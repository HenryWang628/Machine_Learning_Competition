{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "df_train = pd.read_csv(path+'train.csv',\n",
    "converters={'unit_sales':lambda u: np.log1p(float(u)) if float(u) > 0 else 0},parse_dates=[\"date\"])\n",
    "df_test  = pd.read_csv(path + \"test.csv\",parse_dates=[\"date\"])\n",
    "items = pd.read_csv(path+'items.csv')\n",
    "stores = pd.read_csv(path+'stores.csv')\n",
    "# 类型转换\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype(bool)\n",
    "df_test['onpromotion'] = df_test['onpromotion'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2015,12,1)] \n",
    "del df_train\n",
    "\n",
    "df_2017 = df_2017.merge(items, on='item_nbr', how='left')\n",
    "df_2017 = df_2017.merge(stores, on='store_nbr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_2017[df_2017['date']=='2016-12-26']\n",
    "tmp['date'] = '2016-12-25'\n",
    "df_2017 = pd.concat([df_2017, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_train = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = df_test.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "promo_2017 = promo_2017.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"city\", \"class\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1) \n",
    "\n",
    "# df_2017_item = df_2017.groupby('item_nbr')[df_2017.columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(df, dt, forward_steps, periods, freq='D'):\n",
    "    return df[pd.date_range(start=dt-timedelta(days=forward_steps), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        # 视点前 N日促销次数\n",
    "        'promo_3_2017': get_date_range(promo_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        'promo_7_2017': get_date_range(promo_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        'promo_14_2017': get_date_range(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        # 预测集一年前的16日统计销量\n",
    "        \"last_year_mean\": get_date_range(df_2017, t2017, 365, 16).mean(axis=1).values,\n",
    "        \"last_year_meidan\": get_date_range(df_2017, t2017, 365, 16).median(axis=1).values,\n",
    "        \"last_year_max\": get_date_range(df_2017, t2017, 365, 16).max(axis=1).values,\n",
    "        \"last_year_min\": get_date_range(df_2017, t2017, 365, 16).min(axis=1).values,\n",
    "        # 预测集一年前的16日0销次数\n",
    "        \"last_year_count0\": (get_date_range(df_2017, t2017, 365, 16)==0).sum(axis=1).values,\n",
    "        # 预测集一年前的16日促销次数\n",
    "        \"last_year_promo\": get_date_range(promo_2017, t2017, 365, 16).sum(axis=1).values\n",
    "    })\n",
    "    \n",
    "    for i in range(1,8):\n",
    "        # 历史平移，前 N天的销量\n",
    "        X[\"day_{}_hist\".format(i)] = get_date_range(df_2017, t2017, i, 1).values.ravel()\n",
    "        X[\"day_{}_hist_haspromo\".format(i)] = get_date_range(df_2017, t2017, i, 1)[get_date_range(promo_2017, t2017, i, i)==1].values.ravel()\n",
    "        X[\"day_{}_hist_nopromo\".format(i)] = get_date_range(df_2017, t2017, i, 1)[get_date_range(promo_2017, t2017, i, i)==0].values.ravel()\n",
    "        \n",
    "    \n",
    "    for i in [3,5,7,14,21,30,60,90,150,365]:\n",
    "        for d in [0,7,14]:\n",
    "            # 窗口统计，销量 diff/mean/meidan/max/min/std\n",
    "            X['before_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(-1,axis=1).mean(axis=1).values\n",
    "            \n",
    "            X['before_diff_{}_day_decay_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(1,axis=1).apply(lambda x: x * 0.9 ** np.arange(len(x))[::-1]).mean(axis=1).values\n",
    "            X['after_diff_{}_day_decay_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(-1,axis=1).apply(lambda x: x * 0.9 ** np.arange(len(x))[::-1]).mean(axis=1).values\n",
    "            \n",
    "            X['mean_%s_decay_1' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_2' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.7, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_3' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.5, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            \n",
    "            X['mean_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).mean(axis=1).values\n",
    "            X['median_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).median(axis=1).values\n",
    "            X['max_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).max(axis=1).values\n",
    "            X['min_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).min(axis=1).values\n",
    "            X['std_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).std(axis=1).values\n",
    "            X['sum_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).sum(axis=1).values\n",
    "            X['skew_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).skew(axis=1).values\n",
    "\n",
    "        # 有/无促销的时间，销量统计\n",
    "        X['mean_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].mean(axis=1).values\n",
    "        X['median_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].median(axis=1).values\n",
    "        X['max_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].max(axis=1).values\n",
    "        X['min_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].min(axis=1).values\n",
    "        X['std_{}_day_hasnopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].std(axis=1).values\n",
    "        X['sum_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].sum(axis=1).values\n",
    "        X['skew_{}_day_hasnopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].skew(axis=1).values\n",
    "        \n",
    "        X['mean_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].mean(axis=1).values\n",
    "        X['median_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].median(axis=1).values\n",
    "        X['max_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].max(axis=1).values\n",
    "        X['min_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].min(axis=1).values\n",
    "        X['std_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].std(axis=1).values\n",
    "        X['sum_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].sum(axis=1).values\n",
    "        X['skew_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].skew(axis=1).values\n",
    "\n",
    "        # 无销量次数与促销次数\n",
    "        X['nopromo_counts_{}_2017'.format(i)] = (get_date_range(df_2017, t2017, i, i)==0).sum(axis=1).values\n",
    "        X['promo_counts_{}_2017'.format(i)] = get_date_range(promo_2017, t2017, i, i).sum(axis=1).values\n",
    "        \n",
    "        \n",
    "        # 预先按行聚合进行统计\n",
    "        type_mean = get_date_range(df_2017, t2017, i, i).mean(axis=1)\n",
    "#         type_median = get_date_range(df_2017, t2017, i, i).median(axis=1)\n",
    "#         type_max = get_date_range(df_2017, t2017, i, i).max(axis=1)\n",
    "#         type_min = get_date_range(df_2017, t2017, i, i).min(axis=1)\n",
    "#         type_std = get_date_range(df_2017, t2017, i, i).std(axis=1)\n",
    "        \n",
    "        # 统计类型映射\n",
    "        type_map = {0:'mean',1:'median',2:'max',3:'min',4:'std'}\n",
    "        # 不同粒度的统计\n",
    "        for keys in [['item_nbr'],['store_nbr'],['city'],['class'],['store_nbr','class'],['city','class']]:\n",
    "            colname = '_'.join(keys)\n",
    "            for m, tmp_type in enumerate([type_mean]):\n",
    "                tmp = tmp_type.groupby(keys).mean().to_frame('{}_{}_mean'.format(colname,m))\n",
    "                X['{}_{}_{}_mean'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_mean'.format(colname,m)].values   \n",
    "                tmp = tmp_type.groupby(keys).median().to_frame('{}_{}_median'.format(colname,m))\n",
    "                X['{}_{}_{}_median'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_median'.format(colname,m)].values   \n",
    "                tmp = tmp_type.groupby(keys).max().to_frame('{}_{}_max'.format(colname,m))\n",
    "                X['{}_{}_{}_max'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_max'.format(colname,m)].values   \n",
    "                tmp = tmp_type.groupby(keys).min().to_frame('{}_{}_min'.format(colname,m))\n",
    "                X['{}_{}_{}_min'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_min'.format(colname,m)].values   \n",
    "                tmp = tmp_type.groupby(keys).std().to_frame('{}_{}_std'.format(colname,m))\n",
    "                X['{}_{}_{}_std'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_std'.format(colname,m)].values\n",
    "                tmp = tmp_type.groupby(keys).sum().to_frame('{}_{}_sum'.format(colname,m))\n",
    "                X['{}_{}_{}_sum'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_sum'.format(colname,m)].values   \n",
    "                tmp = tmp_type.groupby(keys).skew().to_frame('{}_{}_skew'.format(colname,m))\n",
    "                X['{}_{}_{}_skew'.format(colname,i,m)] = df_2017.join(tmp)['{}_{}_skew'.format(colname,m)].values\n",
    "                               \n",
    "        \n",
    "    for i in range(7):\n",
    "        # 前 N 周平均每周 i 的销量\n",
    "        for periods in [5,10,15,20]:\n",
    "            steps = periods * 7\n",
    "            X['before_diff_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').mean(axis=1).values\n",
    "            X['median_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').median(axis=1).values\n",
    "            X['max_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').max(axis=1).values\n",
    "            X['min_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').min(axis=1).values\n",
    "            X['std_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').std(axis=1).values\n",
    "            X['sum_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').sum(axis=1).values\n",
    "            X['skew_{}_dow{}'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').skew(axis=1).values\n",
    "            \n",
    "            X['before_diff_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].mean(axis=1).values\n",
    "            X['median_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].median(axis=1).values\n",
    "            X['max_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].max(axis=1).values\n",
    "            X['min_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].min(axis=1).values\n",
    "            X['std_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].std(axis=1).values\n",
    "            X['sum_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].sum(axis=1).values\n",
    "            X['skew_{}_dow{}_hasnopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==1].skew(axis=1).values\n",
    "            \n",
    "            X['before_diff_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].mean(axis=1).values\n",
    "            X['median_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].median(axis=1).values\n",
    "            X['max_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].max(axis=1).values\n",
    "            X['min_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].min(axis=1).values\n",
    "            X['std_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].std(axis=1).values\n",
    "            X['sum_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].sum(axis=1).values\n",
    "            X['skew_{}_dow{}_nopromo'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D')[get_date_range(promo_2017, t2017, i, i)==0].skew(axis=1).values\n",
    "            \n",
    "            \n",
    "    for i in range(16):\n",
    "        # 未来16天是否促销日\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=i))].values.astype(np.uint8)\n",
    "        X[\"promo_bef_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=-i))].values.astype(np.uint8)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [7:52:58<00:00, 1236.42s/it]\n"
     ]
    }
   ],
   "source": [
    "X_l, y_l = [], []\n",
    "t2017 = date(2017, 7, 5)\n",
    "n_range = 25\n",
    "for i in tqdm(range(n_range)):\n",
    "    \n",
    "    X_tmp, y_tmp = prepare_dataset(t2017 - timedelta(days=7 * i))\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "\n",
    "#【验证集】7.26\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "#【测试集】8.16\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv('X_train.csv', index=False)\n",
    "# y_df = pd.DataFrame(y_train)\n",
    "# y_df.to_csv('y_train.csv', index=False)\n",
    "\n",
    "# X_train = pd.read_csv('X_train.csv')\n",
    "# y_train = pd.read_csv('y_train.csv')\n",
    "# y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del promo_2017_train\n",
    "del promo_2017_test\n",
    "del promo_2017\n",
    "del X_tmp, y_tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Step 1 ======\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'num_leaves': 80,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 200,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'bagging_freq': 1,\n",
    "    'metric': 'l2',\n",
    "    'n_jobs': 24,\n",
    "    'num_threads': -1\n",
    "}\n",
    "\n",
    "MAX_ROUNDS = 5000\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "item_perishable_dict = dict(zip(items['item_nbr'],items['perishable'].values))\n",
    "train_weight = []\n",
    "val_weight = []\n",
    "\n",
    "items_ = df_2017.reset_index()['item_nbr'].tolist() * n_range\n",
    "for item in items_:\n",
    "    train_weight.append(item_perishable_dict[item] * 0.25 + 1)\n",
    "\n",
    "items_ = df_2017.reset_index()['item_nbr'].values\n",
    "for item in items_:\n",
    "    val_weight.append(item_perishable_dict[item] * 0.25 + 1)\n",
    "\n",
    "del df_2017\n",
    "gc.collect()\n",
    "\n",
    "for i in range(16):\n",
    "\n",
    "    print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i], weight=train_weight\n",
    "    )\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain, weight=val_weight\n",
    "    )\n",
    "    \n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "        valid_sets=[dtrain, dval], verbose_eval=100)\n",
    "    \n",
    "    val_pred.append(bst.predict(X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    \n",
    "    if i == 0:\n",
    "        imps = sorted(zip(X_train.columns, bst.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True)\n",
    "        print(list(sorted(zip(X_train.columns, bst.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "        \n",
    "# 0.279288 0.310069\n",
    "# 0.278647 0.309754\n",
    "# 0.27667  0.310175\n",
    "# 0.276117 0.309686\n",
    "# 0.274491 0.307200 sale_lgb_9\n",
    "# 0.272708 0.305561 sale_lgb_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_500 = [items[0] for items in imps[:500]]\n",
    "\n",
    "# for i in range(1):\n",
    "\n",
    "#     print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "#     dtrain = lgb.Dataset(\n",
    "#         X_train[top_500], label=y_train[:, i], weight=train_weight\n",
    "#     )\n",
    "#     dval = lgb.Dataset(\n",
    "#         X_val[top_500], label=y_val[:, i], reference=dtrain, weight=val_weight\n",
    "#     )\n",
    "#     bst = lgb.train(\n",
    "#         params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "#         valid_sets=[dtrain, dval], verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_1000 = [items[0] for items in imps[:1000]]\n",
    "\n",
    "# for i in range(1):\n",
    "\n",
    "#     print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "#     dtrain = lgb.Dataset(\n",
    "#         X_train[top_1000], label=y_train[:, i], weight=train_weight\n",
    "#     )\n",
    "#     dval = lgb.Dataset(\n",
    "#         X_val[top_1000], label=y_val[:, i], reference=dtrain, weight=val_weight\n",
    "#     )\n",
    "#     bst = lgb.train(\n",
    "#         params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "#         valid_sets=[dtrain, dval], verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_2000 = [items[0] for items in imps[:2000]]\n",
    "\n",
    "# for i in range(1):\n",
    "\n",
    "#     print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "#     dtrain = lgb.Dataset(\n",
    "#         X_train[top_2000], label=y_train[:, i], weight=train_weight\n",
    "#     )\n",
    "#     dval = lgb.Dataset(\n",
    "#         X_val[top_2000], label=y_val[:, i], reference=dtrain, weight=val_weight\n",
    "#     )\n",
    "#     bst = lgb.train(\n",
    "#         params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "#         valid_sets=[dtrain, dval], verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail_2000 = [items[0] for items in imps[-2000:]]\n",
    "\n",
    "# for i in range(1):\n",
    "\n",
    "#     print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "#     dtrain = lgb.Dataset(\n",
    "#         X_train[tail_2000], label=y_train[:, i], weight=train_weight\n",
    "#     )\n",
    "#     dval = lgb.Dataset(\n",
    "#         X_val[tail_2000], label=y_val[:, i], reference=dtrain, weight=val_weight\n",
    "#     )\n",
    "#     bst = lgb.train(\n",
    "#         params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "#         valid_sets=[dtrain, dval], verbose_eval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots(figsize=(15,10))\n",
    "# lgb.plot_importance(bst, max_num_features=20, ax=ax,importance_type='gain')\n",
    "# plt.yticks(fontsize=8)\n",
    "# plt.xlabel('Feature importance',fontsize=14)\n",
    "# plt.ylabel('Features',fontsize=14)\n",
    "# plt.savefig(\"12_feature_importance.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集 mse: 0.3383982506391189\n"
     ]
    }
   ],
   "source": [
    "print(\"验证集 mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))\n",
    "\n",
    "y_test = np.array(test_pred).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_2017.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame('unit_sales')\n",
    "df_preds = df_preds.reset_index()\n",
    "df_preds.columns = ['store_nbr', 'item_nbr', 'city', 'class', 'date', 'unit_sales']\n",
    "\n",
    "submission = df_test[['id','date','store_nbr','item_nbr']].merge(df_preds, on=['date','store_nbr','item_nbr'], how='left').fillna(0)\n",
    "submission['unit_sales'] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission[['id','unit_sales']].to_csv('sale_lgb_10.csv', float_format='%.4f', index=None)\n",
    "\n",
    "# 0.3479347767794617\n",
    "# 0.3436085951363862\n",
    "# 0.3420840578048175\n",
    "# 0.3417190536788496\n",
    "# 0.3388075841917157 sale_lgb_8.csv\n",
    "# 0.3383982506391189 sale_lgb_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集 mse: 0.3383982506391189\n"
     ]
    }
   ],
   "source": [
    "print(\"验证集 mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_lgb1 = pd.read_csv('sale_lgb_98.csv')\n",
    "sale_lgb2 = pd.read_csv('sale_lgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105575</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497035</th>\n",
       "      <td>125497035</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2089339</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497036</th>\n",
       "      <td>125497036</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2106464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497037</th>\n",
       "      <td>125497037</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2110456</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497038</th>\n",
       "      <td>125497038</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2113914</td>\n",
       "      <td>198.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497039</th>\n",
       "      <td>125497039</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2116416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125497040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id        date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "0                  0  2013-01-01         25    103665         7.0         NaN\n",
       "1                  1  2013-01-01         25    105574         1.0         NaN\n",
       "2                  2  2013-01-01         25    105575         2.0         NaN\n",
       "3                  3  2013-01-01         25    108079         1.0         NaN\n",
       "4                  4  2013-01-01         25    108701         1.0         NaN\n",
       "...              ...         ...        ...       ...         ...         ...\n",
       "125497035  125497035  2017-08-15         54   2089339         4.0       False\n",
       "125497036  125497036  2017-08-15         54   2106464         1.0        True\n",
       "125497037  125497037  2017-08-15         54   2110456       192.0       False\n",
       "125497038  125497038  2017-08-15         54   2113914       198.0        True\n",
       "125497039  125497039  2017-08-15         54   2116416         2.0       False\n",
       "\n",
       "[125497040 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
