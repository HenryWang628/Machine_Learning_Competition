{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM,Conv1D, Input, Dense, Add, Multiply\n",
    "from keras import callbacks\n",
    "from keras import optimizers,models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "df_train = pd.read_csv(path+'train.csv',\n",
    "converters={'unit_sales':lambda u: np.log1p(float(u)) if float(u) > 0 else 0},parse_dates=[\"date\"])\n",
    "df_test  = pd.read_csv(path + \"test.csv\",parse_dates=[\"date\"])\n",
    "items = pd.read_csv(path+'items.csv')\n",
    "stores = pd.read_csv(path+'stores.csv')\n",
    "# 类型转换\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype(bool)\n",
    "df_test['onpromotion'] = df_test['onpromotion'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2015,12,1)] \n",
    "del df_train\n",
    "\n",
    "df_2017 = df_2017.merge(items, on='item_nbr', how='left')\n",
    "df_2017 = df_2017.merge(stores, on='store_nbr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_2017[df_2017['date']=='2016-12-26']\n",
    "tmp['date'] = '2016-12-25'\n",
    "df_2017 = pd.concat([df_2017, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_train = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = df_test.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "promo_2017 = promo_2017.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"city\", \"class\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(df, dt, forward_steps, periods, freq='D'):\n",
    "    return df[pd.date_range(start=dt-timedelta(days=forward_steps), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        # 视点前 N日促销次数\n",
    "        'promo_3_2017': get_date_range(promo_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        'promo_7_2017': get_date_range(promo_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        'promo_14_2017': get_date_range(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        # 预测集一年前的16日统计销量\n",
    "        \"last_year_mean\": get_date_range(df_2017, t2017, 365, 16).mean(axis=1).values,\n",
    "        \"last_year_meidan\": get_date_range(df_2017, t2017, 365, 16).median(axis=1).values,\n",
    "        \"last_year_max\": get_date_range(df_2017, t2017, 365, 16).max(axis=1).values,\n",
    "        \"last_year_min\": get_date_range(df_2017, t2017, 365, 16).min(axis=1).values,\n",
    "        # 预测集一年前的16日0销次数\n",
    "        \"last_year_count0\": (get_date_range(df_2017, t2017, 365, 16)==0).sum(axis=1).values,\n",
    "        # 预测集一年前的16日促销次数\n",
    "        \"last_year_promo\": get_date_range(promo_2017, t2017, 365, 16).sum(axis=1).values\n",
    "    })\n",
    "    \n",
    "    for i in range(1,8):\n",
    "        # 历史平移，前 N天的销量\n",
    "        X[\"day_{}_hist\".format(i)] = get_date_range(df_2017, t2017, i, 1).values.ravel()\n",
    "        \n",
    "    \n",
    "    for i in [3,5,7,14,21,30,60,90,150,365]:\n",
    "        for d in [0,7,14]:\n",
    "            # 窗口统计，销量 diff/mean/meidan/max/min/std\n",
    "            X['before_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_%s_decay_1' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_2' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.7, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_3' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.5, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).mean(axis=1).values\n",
    "            X['median_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).median(axis=1).values\n",
    "            X['max_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).max(axis=1).values\n",
    "            X['min_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).min(axis=1).values\n",
    "            X['std_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).std(axis=1).values\n",
    "\n",
    "        # 有/无促销的时间，销量统计\n",
    "        X['mean_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].mean(axis=1).values\n",
    "        X['median_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].median(axis=1).values\n",
    "        X['max_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].max(axis=1).values\n",
    "        X['min_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].min(axis=1).values\n",
    "        X['std_{}_day_hasnopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].std(axis=1).values\n",
    "\n",
    "        X['mean_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].mean(axis=1).values\n",
    "        X['median_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].median(axis=1).values\n",
    "        X['max_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].max(axis=1).values\n",
    "        X['min_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].min(axis=1).values\n",
    "        X['std_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].std(axis=1).values\n",
    "\n",
    "        # 无销量次数与促销次数\n",
    "        X['count0_{}_2017'.format(i)] = (get_date_range(df_2017, t2017, i, i)==0).sum(axis=1).values\n",
    "        X['promo_{}_2017'.format(i)] = get_date_range(promo_2017, t2017, i, i).sum(axis=1).values\n",
    "                               \n",
    "        \n",
    "    for i in range(7):\n",
    "        # 前 N 周平均每周 i 的销量\n",
    "        for periods in [5,10,15,20]:\n",
    "            steps = periods * 7\n",
    "            X['before_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').mean(axis=1).values\n",
    "            X['median_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').median(axis=1).values\n",
    "            X['max_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').max(axis=1).values\n",
    "            X['min_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').min(axis=1).values\n",
    "            X['std_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').std(axis=1).values\n",
    "        \n",
    "        \n",
    "    for i in range(16):\n",
    "        # 未来16天是否促销日\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=i))].values.astype(np.uint8)\n",
    "        X[\"promo_bef_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=-i))].values.astype(np.uint8)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [59:44<00:00, 102.31s/it]\n"
     ]
    }
   ],
   "source": [
    "#【训练集】以7月5日后的16天为最后一个训练集窗口，依次向前递推14周得到14个训练窗口的训练数据\n",
    "X_l, y_l = [], []\n",
    "t2017 = date(2017, 7, 5)\n",
    "n_range = 25\n",
    "for i in tqdm(range(n_range)):\n",
    "    \n",
    "    X_tmp, y_tmp = prepare_dataset(t2017 - timedelta(days=7 * i))\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "\n",
    "#【验证集】7.26\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "#【测试集】8.16\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(shape_):\n",
    "    \n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (shape_))\n",
    "    \n",
    "    x = wave_block(inp, 32, 3, 8)\n",
    "    x = wave_block(x, 64, 3, 4)\n",
    "    x = wave_block(x, 128, 3, 1)\n",
    "    \n",
    "    out = Dense(1, name = 'out')(x)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_test[:] = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Step 1 ======\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "4308475/4308475 [==============================] - 987s 229us/step - loss: 0.4670 - mean_squared_error: 0.4398 - val_loss: 0.3002 - val_mean_squared_error: 0.3002\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 997s 231us/step - loss: 0.3248 - mean_squared_error: 0.3077 - val_loss: 0.2921 - val_mean_squared_error: 0.2921\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 989s 229us/step - loss: 0.3181 - mean_squared_error: 0.3017 - val_loss: 0.2874 - val_mean_squared_error: 0.2874\n",
      "====== Step 2 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 849s 197us/step - loss: 0.3794 - mean_squared_error: 0.3575 - val_loss: 0.3202 - val_mean_squared_error: 0.3202\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 806s 187us/step - loss: 0.3436 - mean_squared_error: 0.3241 - val_loss: 0.3155 - val_mean_squared_error: 0.3155\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 818s 190us/step - loss: 0.3403 - mean_squared_error: 0.3210 - val_loss: 0.3131 - val_mean_squared_error: 0.3131\n",
      "====== Step 3 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 901s 209us/step - loss: 0.3942 - mean_squared_error: 0.3725 - val_loss: 0.3347 - val_mean_squared_error: 0.3347\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 859s 199us/step - loss: 0.3471 - mean_squared_error: 0.3288 - val_loss: 0.3278 - val_mean_squared_error: 0.3278\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 860s 200us/step - loss: 0.3434 - mean_squared_error: 0.3255 - val_loss: 0.3266 - val_mean_squared_error: 0.3266\n",
      "====== Step 4 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 932s 216us/step - loss: 0.4689 - mean_squared_error: 0.4428 - val_loss: 0.3510 - val_mean_squared_error: 0.3510\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 932s 216us/step - loss: 0.3760 - mean_squared_error: 0.3554 - val_loss: 0.3454 - val_mean_squared_error: 0.3454\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 911s 211us/step - loss: 0.3714 - mean_squared_error: 0.3510 - val_loss: 0.3428 - val_mean_squared_error: 0.3428\n",
      "====== Step 5 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 956s 222us/step - loss: 0.4459 - mean_squared_error: 0.4206 - val_loss: 0.3542 - val_mean_squared_error: 0.3542\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 953s 221us/step - loss: 0.3825 - mean_squared_error: 0.3614 - val_loss: 0.3474 - val_mean_squared_error: 0.3474\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 958s 222us/step - loss: 0.3780 - mean_squared_error: 0.3572 - val_loss: 0.3443 - val_mean_squared_error: 0.3443\n",
      "====== Step 6 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 902s 209us/step - loss: 0.4500 - mean_squared_error: 0.4235 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 904s 210us/step - loss: 0.3855 - mean_squared_error: 0.3630 - val_loss: 0.3539 - val_mean_squared_error: 0.3539\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 944s 219us/step - loss: 0.3814 - mean_squared_error: 0.3592 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "====== Step 7 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 929s 216us/step - loss: 0.4614 - mean_squared_error: 0.4341 - val_loss: 0.4140 - val_mean_squared_error: 0.4140\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 953s 221us/step - loss: 0.3750 - mean_squared_error: 0.3539 - val_loss: 0.4064 - val_mean_squared_error: 0.4064\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 942s 219us/step - loss: 0.3703 - mean_squared_error: 0.3496 - val_loss: 0.4084 - val_mean_squared_error: 0.4084\n",
      "====== Step 8 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 924s 214us/step - loss: 0.4564 - mean_squared_error: 0.4296 - val_loss: 0.3861 - val_mean_squared_error: 0.3861\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 903s 210us/step - loss: 0.3665 - mean_squared_error: 0.3472 - val_loss: 0.3771 - val_mean_squared_error: 0.3771\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 909s 211us/step - loss: 0.3592 - mean_squared_error: 0.3408 - val_loss: 0.3759 - val_mean_squared_error: 0.3759\n",
      "====== Step 9 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 962s 223us/step - loss: 0.4342 - mean_squared_error: 0.4091 - val_loss: 0.3734 - val_mean_squared_error: 0.3734\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 942s 219us/step - loss: 0.3753 - mean_squared_error: 0.3540 - val_loss: 0.3671 - val_mean_squared_error: 0.3671\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 949s 220us/step - loss: 0.3712 - mean_squared_error: 0.3503 - val_loss: 0.3657 - val_mean_squared_error: 0.3657\n",
      "====== Step 10 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 946s 220us/step - loss: 0.5078 - mean_squared_error: 0.4781 - val_loss: 0.3758 - val_mean_squared_error: 0.3758\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 915s 212us/step - loss: 0.3878 - mean_squared_error: 0.3669 - val_loss: 0.3683 - val_mean_squared_error: 0.3683\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 935s 217us/step - loss: 0.3799 - mean_squared_error: 0.3596 - val_loss: 0.3617 - val_mean_squared_error: 0.3617\n",
      "====== Step 11 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 1007s 234us/step - loss: 0.4749 - mean_squared_error: 0.4483 - val_loss: 0.3734 - val_mean_squared_error: 0.3734\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 1213s 282us/step - loss: 0.4095 - mean_squared_error: 0.3871 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 995s 231us/step - loss: 0.4042 - mean_squared_error: 0.3822 - val_loss: 0.3619 - val_mean_squared_error: 0.3619\n",
      "====== Step 12 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 1260s 293us/step - loss: 0.4795 - mean_squared_error: 0.4524 - val_loss: 0.3846 - val_mean_squared_error: 0.3846\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 3223s 748us/step - loss: 0.4179 - mean_squared_error: 0.3948 - val_loss: 0.3805 - val_mean_squared_error: 0.3805\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 2904s 674us/step - loss: 0.4123 - mean_squared_error: 0.3896 - val_loss: 0.3790 - val_mean_squared_error: 0.3790\n",
      "====== Step 13 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 925s 215us/step - loss: 0.4803 - mean_squared_error: 0.4516 - val_loss: 0.3746 - val_mean_squared_error: 0.3746\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 888s 206us/step - loss: 0.4123 - mean_squared_error: 0.3881 - val_loss: 0.3672 - val_mean_squared_error: 0.3672\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 905s 210us/step - loss: 0.4069 - mean_squared_error: 0.3832 - val_loss: 0.3648 - val_mean_squared_error: 0.3648\n",
      "====== Step 14 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 965s 224us/step - loss: 0.4504 - mean_squared_error: 0.4241 - val_loss: 0.3608 - val_mean_squared_error: 0.3608\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 971s 225us/step - loss: 0.3941 - mean_squared_error: 0.3718 - val_loss: 0.3561 - val_mean_squared_error: 0.3561\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 957s 222us/step - loss: 0.3895 - mean_squared_error: 0.3677 - val_loss: 0.3526 - val_mean_squared_error: 0.3526\n",
      "====== Step 15 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 1020s 237us/step - loss: 0.5635 - mean_squared_error: 0.5301 - val_loss: 0.3611 - val_mean_squared_error: 0.3611\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 1011s 235us/step - loss: 0.3924 - mean_squared_error: 0.3715 - val_loss: 0.3495 - val_mean_squared_error: 0.3495\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 1018s 236us/step - loss: 0.3834 - mean_squared_error: 0.3634 - val_loss: 0.3437 - val_mean_squared_error: 0.3437\n",
      "====== Step 16 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/3\n",
      "4308475/4308475 [==============================] - 970s 225us/step - loss: 0.4462 - mean_squared_error: 0.4201 - val_loss: 0.3686 - val_mean_squared_error: 0.3686\n",
      "Epoch 2/3\n",
      "4308475/4308475 [==============================] - 942s 219us/step - loss: 0.3950 - mean_squared_error: 0.3724 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "Epoch 3/3\n",
      "4308475/4308475 [==============================] - 939s 218us/step - loss: 0.3910 - mean_squared_error: 0.3688 - val_loss: 0.3635 - val_mean_squared_error: 0.3635\n"
     ]
    }
   ],
   "source": [
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "item_perishable_dict = dict(zip(items['item_nbr'],items['perishable'].values))\n",
    "train_weight = []\n",
    "val_weight = []\n",
    "\n",
    "items_ = df_2017.reset_index()['item_nbr'].tolist() * n_range\n",
    "for item in items_:\n",
    "    train_weight.append(item_perishable_dict[item] * 0.25 + 1)\n",
    "\n",
    "for i in range(16):\n",
    "\n",
    "    print(\"====== Step %d ======\" % (i+1))\n",
    "    \n",
    "    # 编译部分\n",
    "    shape_ = (None, X_train.shape[2])\n",
    "    model = build_model(shape_)\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(lr=0.001), metrics=['mse'])\n",
    "    # 回调函数\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='min')\n",
    "    callbacks = [reduce_lr, earlystopping]\n",
    "    # 训练部分\n",
    "    model.fit(X_train, y_train[:, i].reshape((y_train.shape[0], 1, 1)), batch_size = 8192*2, epochs = 3, verbose=1,\n",
    "              sample_weight=np.array(train_weight), validation_data=(X_val, y_val[:, i].reshape((y_val.shape[0], 1, 1))), \n",
    "              callbacks=callbacks, shuffle=True)\n",
    "\n",
    "    val_pred.append(model.predict(X_val))\n",
    "    test_pred.append(model.predict(X_test))\n",
    "        \n",
    "# 0.279288 0.310069\n",
    "# 0.278647 0.309754\n",
    "# 0.27667  0.310175\n",
    "# 0.276117 0.309686\n",
    "# 0.274491 0.307200 sale_lgb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172339, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred[0].transpose()[0].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172339, 16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([val_pred[i].transpose()[0].transpose() for i in range(16)]).transpose()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集 mse: 0.3527629813294579\n"
     ]
    }
   ],
   "source": [
    "print(\"验证集 mse:\", mean_squared_error(\n",
    "    y_val, np.array([val_pred[i].transpose()[0].transpose() for i in range(16)]).transpose()[0]))\n",
    "\n",
    "y_test = np.array([test_pred[i].transpose()[0].transpose() for i in range(16)]).transpose()[0]\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_2017.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame('unit_sales')\n",
    "df_preds = df_preds.reset_index()\n",
    "df_preds.columns = ['store_nbr', 'item_nbr', 'city', 'class', 'date', 'unit_sales']\n",
    "\n",
    "submission = df_test[['id','date','store_nbr','item_nbr']].merge(df_preds, on=['date','store_nbr','item_nbr'], how='left').fillna(0)\n",
    "submission['unit_sales'] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission[['id','unit_sales']].to_csv('sale_wavenet_1.csv', float_format='%.4f', index=None)\n",
    "\n",
    "# 0.3479347767794617\n",
    "# 0.3436085951363862\n",
    "# 0.3420840578048175\n",
    "# 0.3417190536788496\n",
    "# 0.3388075841917157 sale_lgb_8.csv\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_lgb1 = pd.read_csv('sale_lgb_98.csv')\n",
    "sale_lgb2 = pd.read_csv('sale_lgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_lgb1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_lgb2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>103665</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>105575</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>25</td>\n",
       "      <td>108701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497035</th>\n",
       "      <td>125497035</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2089339</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497036</th>\n",
       "      <td>125497036</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2106464</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497037</th>\n",
       "      <td>125497037</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2110456</td>\n",
       "      <td>192.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497038</th>\n",
       "      <td>125497038</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2113914</td>\n",
       "      <td>198.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497039</th>\n",
       "      <td>125497039</td>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2116416</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125497040 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id        date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "0                  0  2013-01-01         25    103665         7.0         NaN\n",
       "1                  1  2013-01-01         25    105574         1.0         NaN\n",
       "2                  2  2013-01-01         25    105575         2.0         NaN\n",
       "3                  3  2013-01-01         25    108079         1.0         NaN\n",
       "4                  4  2013-01-01         25    108701         1.0         NaN\n",
       "...              ...         ...        ...       ...         ...         ...\n",
       "125497035  125497035  2017-08-15         54   2089339         4.0       False\n",
       "125497036  125497036  2017-08-15         54   2106464         1.0        True\n",
       "125497037  125497037  2017-08-15         54   2110456       192.0       False\n",
       "125497038  125497038  2017-08-15         54   2113914       198.0        True\n",
       "125497039  125497039  2017-08-15         54   2116416         2.0       False\n",
       "\n",
       "[125497040 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
