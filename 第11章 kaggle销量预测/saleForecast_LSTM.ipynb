{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './input/'\n",
    "df_train = pd.read_csv(path+'train.csv',\n",
    "converters={'unit_sales':lambda u: np.log1p(float(u)) if float(u) > 0 else 0},parse_dates=[\"date\"])\n",
    "df_test  = pd.read_csv(path + \"test.csv\",parse_dates=[\"date\"])\n",
    "items = pd.read_csv(path+'items.csv')\n",
    "stores = pd.read_csv(path+'stores.csv')\n",
    "# 类型转换\n",
    "df_train['onpromotion'] = df_train['onpromotion'].astype(bool)\n",
    "df_test['onpromotion'] = df_test['onpromotion'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2015,12,1)] \n",
    "del df_train\n",
    "\n",
    "df_2017 = df_2017.merge(items, on='item_nbr', how='left')\n",
    "df_2017 = df_2017.merge(stores, on='store_nbr', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df_2017[df_2017['date']=='2016-12-26']\n",
    "tmp['date'] = '2016-12-25'\n",
    "df_2017 = pd.concat([df_2017, tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_train = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "\n",
    "promo_2017_test = df_test.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "\n",
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "promo_2017 = promo_2017.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"city\", \"class\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)\n",
    "df_2017.columns = df_2017.columns.get_level_values(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(df, dt, forward_steps, periods, freq='D'):\n",
    "    return df[pd.date_range(start=dt-timedelta(days=forward_steps), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        # 视点前 N日促销次数\n",
    "        'promo_3_2017': get_date_range(promo_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        'promo_7_2017': get_date_range(promo_2017, t2017, 7, 7).sum(axis=1).values,\n",
    "        'promo_14_2017': get_date_range(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        # 预测集一年前的16日统计销量\n",
    "        \"last_year_mean\": get_date_range(df_2017, t2017, 365, 16).mean(axis=1).values,\n",
    "        \"last_year_meidan\": get_date_range(df_2017, t2017, 365, 16).median(axis=1).values,\n",
    "        \"last_year_max\": get_date_range(df_2017, t2017, 365, 16).max(axis=1).values,\n",
    "        \"last_year_min\": get_date_range(df_2017, t2017, 365, 16).min(axis=1).values,\n",
    "        # 预测集一年前的16日0销次数\n",
    "        \"last_year_count0\": (get_date_range(df_2017, t2017, 365, 16)==0).sum(axis=1).values,\n",
    "        # 预测集一年前的16日促销次数\n",
    "        \"last_year_promo\": get_date_range(promo_2017, t2017, 365, 16).sum(axis=1).values\n",
    "    })\n",
    "    \n",
    "    for i in range(1,8):\n",
    "        # 历史平移，前 N天的销量\n",
    "        X[\"day_{}_hist\".format(i)] = get_date_range(df_2017, t2017, i, 1).values.ravel()\n",
    "        \n",
    "    \n",
    "    for i in [3, 4, 5, 6, 7, 10, 14, 21, 30, 60, 90, 120, 140, 356]:\n",
    "        for d in [0,7,14]:\n",
    "            # 窗口统计，销量 diff/mean/meidan/max/min/std\n",
    "            X['before_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_day_mean'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_%s_decay_1' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_2' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.7, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_%s_decay_3' % i] = (get_date_range(df_2017, t2017-timedelta(days=d), i, i) * np.power(0.5, np.arange(i)[::-1])).sum(axis=1).values\n",
    "            X['mean_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).mean(axis=1).values\n",
    "            X['median_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).median(axis=1).values\n",
    "            X['max_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).max(axis=1).values\n",
    "            X['min_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).min(axis=1).values\n",
    "            X['std_{}_day'.format(i)] = get_date_range(df_2017, t2017-timedelta(days=d), i, i).std(axis=1).values\n",
    "\n",
    "        # 有/无促销的时间，销量统计\n",
    "        X['mean_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].mean(axis=1).values\n",
    "        X['median_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].median(axis=1).values\n",
    "        X['max_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].max(axis=1).values\n",
    "        X['min_{}_day_haspromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].min(axis=1).values\n",
    "        X['std_{}_day_hasnopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==1].std(axis=1).values\n",
    "\n",
    "        X['mean_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].mean(axis=1).values\n",
    "        X['median_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].median(axis=1).values\n",
    "        X['max_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].max(axis=1).values\n",
    "        X['min_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].min(axis=1).values\n",
    "        X['std_{}_day_nopromo'.format(i)] = get_date_range(df_2017, t2017, i, i)[get_date_range(promo_2017, t2017, i, i)==0].std(axis=1).values\n",
    "\n",
    "        # 无销量次数与促销次数\n",
    "        X['count0_{}_2017'.format(i)] = (get_date_range(df_2017, t2017, i, i)==0).sum(axis=1).values\n",
    "        X['promo_{}_2017'.format(i)] = get_date_range(promo_2017, t2017, i, i).sum(axis=1).values\n",
    "                               \n",
    "        \n",
    "    for i in range(7):\n",
    "        # 前 N 周平均每周 i 的销量\n",
    "        for periods in [5,10,15,20]:\n",
    "            steps = periods * 7\n",
    "            X['before_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(1,axis=1).mean(axis=1).values\n",
    "            X['after_diff_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').diff(-1,axis=1).mean(axis=1).values\n",
    "            X['mean_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').mean(axis=1).values\n",
    "            X['median_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').median(axis=1).values\n",
    "            X['max_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').max(axis=1).values\n",
    "            X['min_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').min(axis=1).values\n",
    "            X['std_{}_dow{}_2017'.format(periods,i)] = get_date_range(df_2017, t2017, steps-i, periods, freq='7D').std(axis=1).values\n",
    "        \n",
    "        \n",
    "    for i in range(16):\n",
    "        # 未来16天是否促销日\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=i))].values.astype(np.uint8)\n",
    "        X[\"promo_bef_{}\".format(i)] = promo_2017[str(t2017 + timedelta(days=-i))].values.astype(np.uint8)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [46:12<00:00, 114.41s/it]\n"
     ]
    }
   ],
   "source": [
    "#【训练集】以7月5日后的16天为最后一个训练集窗口，依次向前递推14周得到14个训练窗口的训练数据\n",
    "X_l, y_l = [], []\n",
    "t2017 = date(2017, 7, 5)\n",
    "n_range = 25\n",
    "for i in tqdm(range(n_range)):\n",
    "    \n",
    "    X_tmp, y_tmp = prepare_dataset(t2017 - timedelta(days=7 * i))\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "\n",
    "#【验证集】7.26\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "#【测试集】8.16\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(512, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.2))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.1))\n",
    "\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(.05))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(16))\n",
    "    model.add(Activation(activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(pd.concat([X_train, X_val, X_test]))\n",
    "X_train[:] = scaler.transform(X_train)\n",
    "X_val[:] = scaler.transform(X_val)\n",
    "X_test[:] = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.fillna(0)\n",
    "X_val = X_val.fillna(0)\n",
    "X_test = X_test.fillna(0)\n",
    "\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.values.reshape((X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Step 1 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 169s 39us/step - loss: 0.5030 - mean_squared_error: 0.4752 - val_loss: 0.2849 - val_mean_squared_error: 0.2849\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 132s 31us/step - loss: 0.3411 - mean_squared_error: 0.3231 - val_loss: 0.2799 - val_mean_squared_error: 0.2799\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 123s 29us/step - loss: 0.3349 - mean_squared_error: 0.3174 - val_loss: 0.2809 - val_mean_squared_error: 0.2809\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 144s 33us/step - loss: 0.3310 - mean_squared_error: 0.3138 - val_loss: 0.2795 - val_mean_squared_error: 0.2795\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 139s 32us/step - loss: 0.3282 - mean_squared_error: 0.3112 - val_loss: 0.2822 - val_mean_squared_error: 0.2822\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 142s 33us/step - loss: 0.3261 - mean_squared_error: 0.3093 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 135s 31us/step - loss: 0.3247 - mean_squared_error: 0.3080 - val_loss: 0.2800 - val_mean_squared_error: 0.2800\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00007: early stopping\n",
      "====== Step 2 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 148s 34us/step - loss: 0.5210 - mean_squared_error: 0.4921 - val_loss: 0.3193 - val_mean_squared_error: 0.3193\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 140s 33us/step - loss: 0.3713 - mean_squared_error: 0.3498 - val_loss: 0.3168 - val_mean_squared_error: 0.3168\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 154s 36us/step - loss: 0.3620 - mean_squared_error: 0.3412 - val_loss: 0.3106 - val_mean_squared_error: 0.3106\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 147s 34us/step - loss: 0.3573 - mean_squared_error: 0.3368 - val_loss: 0.3116 - val_mean_squared_error: 0.3116\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 155s 36us/step - loss: 0.3544 - mean_squared_error: 0.3342 - val_loss: 0.3135 - val_mean_squared_error: 0.3135\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 140s 33us/step - loss: 0.3524 - mean_squared_error: 0.3323 - val_loss: 0.3115 - val_mean_squared_error: 0.3115\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00006: early stopping\n",
      "====== Step 3 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 142s 33us/step - loss: 0.4868 - mean_squared_error: 0.4599 - val_loss: 0.3275 - val_mean_squared_error: 0.3275\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 132s 31us/step - loss: 0.3709 - mean_squared_error: 0.3510 - val_loss: 0.3265 - val_mean_squared_error: 0.3265\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 133s 31us/step - loss: 0.3635 - mean_squared_error: 0.3442 - val_loss: 0.3291 - val_mean_squared_error: 0.3291\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 125s 29us/step - loss: 0.3597 - mean_squared_error: 0.3406 - val_loss: 0.3275 - val_mean_squared_error: 0.3275\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.3571 - mean_squared_error: 0.3382 - val_loss: 0.3250 - val_mean_squared_error: 0.3250\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 122s 28us/step - loss: 0.3551 - mean_squared_error: 0.3364 - val_loss: 0.3263 - val_mean_squared_error: 0.3263\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.3536 - mean_squared_error: 0.3350 - val_loss: 0.3311 - val_mean_squared_error: 0.3311\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 118s 27us/step - loss: 0.3530 - mean_squared_error: 0.3345 - val_loss: 0.3249 - val_mean_squared_error: 0.3249\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00008: early stopping\n",
      "====== Step 4 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 131s 30us/step - loss: 0.4861 - mean_squared_error: 0.4591 - val_loss: 0.3423 - val_mean_squared_error: 0.3423\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 133s 31us/step - loss: 0.3960 - mean_squared_error: 0.3741 - val_loss: 0.3375 - val_mean_squared_error: 0.3375\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 137s 32us/step - loss: 0.3902 - mean_squared_error: 0.3687 - val_loss: 0.3380 - val_mean_squared_error: 0.3380\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 119s 28us/step - loss: 0.3861 - mean_squared_error: 0.3649 - val_loss: 0.3404 - val_mean_squared_error: 0.3404\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 128s 30us/step - loss: 0.3829 - mean_squared_error: 0.3619 - val_loss: 0.3366 - val_mean_squared_error: 0.3366\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 144s 33us/step - loss: 0.3810 - mean_squared_error: 0.3601 - val_loss: 0.3374 - val_mean_squared_error: 0.3374\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 142s 33us/step - loss: 0.3800 - mean_squared_error: 0.3592 - val_loss: 0.3379 - val_mean_squared_error: 0.3379\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 127s 30us/step - loss: 0.3786 - mean_squared_error: 0.3579 - val_loss: 0.3367 - val_mean_squared_error: 0.3367\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00008: early stopping\n",
      "====== Step 5 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 135s 31us/step - loss: 0.5548 - mean_squared_error: 0.5237 - val_loss: 0.3460 - val_mean_squared_error: 0.3460\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 139s 32us/step - loss: 0.4060 - mean_squared_error: 0.3834 - val_loss: 0.3415 - val_mean_squared_error: 0.3415\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 118s 27us/step - loss: 0.3997 - mean_squared_error: 0.3776 - val_loss: 0.3439 - val_mean_squared_error: 0.3439\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.3952 - mean_squared_error: 0.3735 - val_loss: 0.3415 - val_mean_squared_error: 0.3415\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 126s 29us/step - loss: 0.3915 - mean_squared_error: 0.3701 - val_loss: 0.3379 - val_mean_squared_error: 0.3379\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 119s 28us/step - loss: 0.3894 - mean_squared_error: 0.3681 - val_loss: 0.3383 - val_mean_squared_error: 0.3383\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 124s 29us/step - loss: 0.3879 - mean_squared_error: 0.3667 - val_loss: 0.3412 - val_mean_squared_error: 0.3412\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 118s 27us/step - loss: 0.3867 - mean_squared_error: 0.3656 - val_loss: 0.3398 - val_mean_squared_error: 0.3398\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00008: early stopping\n",
      "====== Step 6 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 131s 31us/step - loss: 0.5233 - mean_squared_error: 0.4930 - val_loss: 0.3487 - val_mean_squared_error: 0.3487\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 122s 28us/step - loss: 0.4040 - mean_squared_error: 0.3803 - val_loss: 0.3460 - val_mean_squared_error: 0.3460\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 137s 32us/step - loss: 0.3977 - mean_squared_error: 0.3745 - val_loss: 0.3444 - val_mean_squared_error: 0.3444\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 134s 31us/step - loss: 0.3949 - mean_squared_error: 0.3720 - val_loss: 0.3501 - val_mean_squared_error: 0.3501\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.3924 - mean_squared_error: 0.3696 - val_loss: 0.3537 - val_mean_squared_error: 0.3537\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 140s 32us/step - loss: 0.3904 - mean_squared_error: 0.3678 - val_loss: 0.3461 - val_mean_squared_error: 0.3461\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00006: early stopping\n",
      "====== Step 7 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 147s 34us/step - loss: 0.4944 - mean_squared_error: 0.4662 - val_loss: 0.4013 - val_mean_squared_error: 0.4013\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 127s 30us/step - loss: 0.3884 - mean_squared_error: 0.3667 - val_loss: 0.4071 - val_mean_squared_error: 0.4071\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.3820 - mean_squared_error: 0.3608 - val_loss: 0.4105 - val_mean_squared_error: 0.4105\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 132s 31us/step - loss: 0.3790 - mean_squared_error: 0.3581 - val_loss: 0.4092 - val_mean_squared_error: 0.4092\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00004: early stopping\n",
      "====== Step 8 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 265s 62us/step - loss: 0.4973 - mean_squared_error: 0.4695 - val_loss: 0.3698 - val_mean_squared_error: 0.3698\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 369s 86us/step - loss: 0.3780 - mean_squared_error: 0.3584 - val_loss: 0.3673 - val_mean_squared_error: 0.3673\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 361s 84us/step - loss: 0.3713 - mean_squared_error: 0.3523 - val_loss: 0.3706 - val_mean_squared_error: 0.3706\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 257s 60us/step - loss: 0.3678 - mean_squared_error: 0.3492 - val_loss: 0.3760 - val_mean_squared_error: 0.3760\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 119s 28us/step - loss: 0.3657 - mean_squared_error: 0.3472 - val_loss: 0.3680 - val_mean_squared_error: 0.3680\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00005: early stopping\n",
      "====== Step 9 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 149s 34us/step - loss: 0.5526 - mean_squared_error: 0.5207 - val_loss: 0.3639 - val_mean_squared_error: 0.3639\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 137s 32us/step - loss: 0.3913 - mean_squared_error: 0.3690 - val_loss: 0.3620 - val_mean_squared_error: 0.3620\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 136s 32us/step - loss: 0.3856 - mean_squared_error: 0.3638 - val_loss: 0.3601 - val_mean_squared_error: 0.3601\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.3825 - mean_squared_error: 0.3610 - val_loss: 0.3574 - val_mean_squared_error: 0.3574\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 127s 29us/step - loss: 0.3804 - mean_squared_error: 0.3590 - val_loss: 0.3704 - val_mean_squared_error: 0.3704\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.3787 - mean_squared_error: 0.3575 - val_loss: 0.3669 - val_mean_squared_error: 0.3669\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.3774 - mean_squared_error: 0.3563 - val_loss: 0.3550 - val_mean_squared_error: 0.3550\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.3762 - mean_squared_error: 0.3552 - val_loss: 0.3638 - val_mean_squared_error: 0.3638\n",
      "Epoch 9/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.3755 - mean_squared_error: 0.3545 - val_loss: 0.3556 - val_mean_squared_error: 0.3556\n",
      "Epoch 10/50\n",
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.3748 - mean_squared_error: 0.3539 - val_loss: 0.3629 - val_mean_squared_error: 0.3629\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00010: early stopping\n",
      "====== Step 10 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 148s 34us/step - loss: 0.5002 - mean_squared_error: 0.4719 - val_loss: 0.3530 - val_mean_squared_error: 0.3530\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 130s 30us/step - loss: 0.3947 - mean_squared_error: 0.3736 - val_loss: 0.3527 - val_mean_squared_error: 0.3527\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.3886 - mean_squared_error: 0.3681 - val_loss: 0.3495 - val_mean_squared_error: 0.3495\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 140s 32us/step - loss: 0.3856 - mean_squared_error: 0.3654 - val_loss: 0.3551 - val_mean_squared_error: 0.3551\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 133s 31us/step - loss: 0.3831 - mean_squared_error: 0.3630 - val_loss: 0.3490 - val_mean_squared_error: 0.3490\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 138s 32us/step - loss: 0.3817 - mean_squared_error: 0.3617 - val_loss: 0.3549 - val_mean_squared_error: 0.3549\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 126s 29us/step - loss: 0.3803 - mean_squared_error: 0.3605 - val_loss: 0.3494 - val_mean_squared_error: 0.3494\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.3791 - mean_squared_error: 0.3594 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00008: early stopping\n",
      "====== Step 11 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 130s 30us/step - loss: 0.6034 - mean_squared_error: 0.5695 - val_loss: 0.3661 - val_mean_squared_error: 0.3661\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.4336 - mean_squared_error: 0.4097 - val_loss: 0.3615 - val_mean_squared_error: 0.3615\n",
      "Epoch 3/50\n",
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.4250 - mean_squared_error: 0.4018 - val_loss: 0.3597 - val_mean_squared_error: 0.3597\n",
      "Epoch 4/50\n",
      "4308475/4308475 [==============================] - 125s 29us/step - loss: 0.4207 - mean_squared_error: 0.3978 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "Epoch 5/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.4171 - mean_squared_error: 0.3945 - val_loss: 0.3623 - val_mean_squared_error: 0.3623\n",
      "Epoch 6/50\n",
      "4308475/4308475 [==============================] - 121s 28us/step - loss: 0.4146 - mean_squared_error: 0.3921 - val_loss: 0.3595 - val_mean_squared_error: 0.3595\n",
      "Epoch 7/50\n",
      "4308475/4308475 [==============================] - 135s 31us/step - loss: 0.4123 - mean_squared_error: 0.3900 - val_loss: 0.3599 - val_mean_squared_error: 0.3599\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.4077 - mean_squared_error: 0.3844 - val_loss: 0.3632 - val_mean_squared_error: 0.3632\n",
      "Epoch 11/50\n",
      "4308475/4308475 [==============================] - 142s 33us/step - loss: 0.4070 - mean_squared_error: 0.3838 - val_loss: 0.3605 - val_mean_squared_error: 0.3605\n",
      "Epoch 12/50\n",
      "4308475/4308475 [==============================] - 141s 33us/step - loss: 0.4052 - mean_squared_error: 0.3821 - val_loss: 0.3633 - val_mean_squared_error: 0.3633\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 00014: early stopping\n",
      "====== Step 14 ======\n",
      "Train on 4308475 samples, validate on 172339 samples\n",
      "Epoch 1/50\n",
      "4308475/4308475 [==============================] - 136s 32us/step - loss: 0.5475 - mean_squared_error: 0.5158 - val_loss: 0.3524 - val_mean_squared_error: 0.3524\n",
      "Epoch 2/50\n",
      "4308475/4308475 [==============================] - 142s 33us/step - loss: 0.4099 - mean_squared_error: 0.3870 - val_loss: 0.3489 - val_mean_squared_error: 0.3489\n",
      "Epoch 3/50\n",
      " 360448/4308475 [=>............................] - ETA: 2:08 - loss: 0.4036 - mean_squared_error: 0.3812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4308475/4308475 [==============================] - 120s 28us/step - loss: 0.3931 - mean_squared_error: 0.3717 - val_loss: 0.3463 - val_mean_squared_error: 0.3463\n",
      "Epoch 8/50\n",
      "4308475/4308475 [==============================] - 132s 31us/step - loss: 0.3922 - mean_squared_error: 0.3708 - val_loss: 0.3497 - val_mean_squared_error: 0.3497\n",
      "Epoch 9/50\n",
      "2732032/4308475 [==================>...........] - ETA: 51s - loss: 0.3904 - mean_squared_error: 0.3691"
     ]
    }
   ],
   "source": [
    "val_pred = []\n",
    "test_pred = []\n",
    "\n",
    "item_perishable_dict = dict(zip(items['item_nbr'],items['perishable'].values))\n",
    "train_weight = []\n",
    "val_weight = []\n",
    "\n",
    "items_ = df_2017.reset_index()['item_nbr'].tolist() * n_range\n",
    "for item in items_:\n",
    "    train_weight.append(item_perishable_dict[item] * 0.25 + 1)\n",
    "\n",
    "for i in range(16):\n",
    "\n",
    "    print(\"====== Step %d ======\" % (i+1))\n",
    "    y_mean = y_train[:, i].mean()\n",
    "    # 编译部分\n",
    "    model = build_model()\n",
    "    # 回调函数\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "    earlystopping = EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='min')\n",
    "    callbacks = [reduce_lr, earlystopping]\n",
    "    # 训练部分\n",
    "    model.fit(X_train, y_train[:, i]-y_mean, batch_size = 4096, epochs = 50, verbose=1,\n",
    "              sample_weight=np.array(train_weight), validation_data=(X_val, y_val[:, i]-y_mean), \n",
    "              callbacks=callbacks, shuffle=True)\n",
    "\n",
    "    val_pred.append(model.predict(X_val)+y_mean)\n",
    "    test_pred.append(model.predict(X_test)+y_mean)\n",
    "        \n",
    "# 0.279288 0.310069\n",
    "# 0.278647 0.309754\n",
    "# 0.27667  0.310175\n",
    "# 0.276117 0.309686\n",
    "# 0.274491 0.307200 sale_lgb_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集 mse: 0.34803971934874334\n"
     ]
    }
   ],
   "source": [
    "print(\"验证集 mse:\", mean_squared_error(\n",
    "    y_val, np.array(val_pred).transpose()[0]))\n",
    "\n",
    "y_test = np.array(test_pred).transpose()[0]\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=df_2017.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame('unit_sales')\n",
    "df_preds = df_preds.reset_index()\n",
    "df_preds.columns = ['store_nbr', 'item_nbr', 'city', 'class', 'date', 'unit_sales']\n",
    "\n",
    "submission = df_test[['id','date','store_nbr','item_nbr']].merge(df_preds, on=['date','store_nbr','item_nbr'], how='left').fillna(0)\n",
    "submission['unit_sales'] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission[['id','unit_sales']].to_csv('sale_lstm_2.csv', float_format='%.4f', index=None)\n",
    "\n",
    "# 0.3479347767794617\n",
    "# 0.3436085951363862\n",
    "# 0.3420840578048175\n",
    "# 0.3417190536788496\n",
    "# 0.3388075841917157 sale_lgb_8.csv\n",
    "# 0.45825374853685075 sale_lstm_1.csv\n",
    "# 0.34803971934874334 sale_lstm_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('sale_lstm_2.csv')\n",
    "sub2 = pd.read_csv('sale_lgb_10.csv')\n",
    "sub3 = pd.read_csv('sale_wavenet_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1['unit_sales'] = sub1['unit_sales'] * 0.2 + sub2['unit_sales'] * 0.7 + sub3['unit_sales'] * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1.to_csv('ronghe2.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
