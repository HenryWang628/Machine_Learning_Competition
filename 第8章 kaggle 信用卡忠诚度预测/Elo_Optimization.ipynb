{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,tqdm_notebook \n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.8 s, sys: 6.4 s, total: 57.2 s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "new_transactions = pd.read_csv('data/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "historical_transactions = pd.read_csv('data/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "for col in ['authorized_flag', 'category_1']:\n",
    "    historical_transactions[col] = historical_transactions[col].map({'Y':1, 'N':0})\n",
    "    new_transactions[col]        = new_transactions[col].map({'Y':1, 'N':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 8.12 s, total: 20.2 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 加载训练集，测试集，基本处理\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "\n",
    "target = train['target']\n",
    "for df in [train, test]:    \n",
    "    df['year']  = df['first_active_month'].fillna('0-0').apply(lambda x:int(str(x).split('-')[0]))\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['elapsed_time'] = (datetime.date(2018,3, 1) - df['first_active_month'].dt.date).dt.days\n",
    "    \n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['dayofyear'] = df['first_active_month'].dt.dayofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    \n",
    "## 交易表合并train test\n",
    "train_test = pd.concat([train[['card_id','first_active_month']], test[['card_id','first_active_month']] ], axis=0, ignore_index=True)\n",
    "historical_transactions   = historical_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n",
    "new_transactions = new_transactions.merge(train_test[['card_id','first_active_month']], on=['card_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3192.83 Mb (64.1% reduction)\n",
      "Mem. usage decreased to 211.55 Mb (64.7% reduction)\n",
      "CPU times: user 6min 14s, sys: 46.1 s, total: 7min\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def month_trans(x): \n",
    "    return x // 30\n",
    "\n",
    "def week_trans(x): \n",
    "    return x // 7\n",
    "\n",
    "## 交易表预处理\n",
    "def get_expand_common(df_):\n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['category_2'].fillna(1.0,inplace=True)\n",
    "    df['category_3'].fillna('A',inplace=True)\n",
    "    df['category_3'] = df['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    df['installments'].replace(-1, np.nan,inplace=True)\n",
    "    df['installments'].replace(999, np.nan,inplace=True)\n",
    "    \n",
    "    df['purchase_amount'] = np.round(df['purchase_amount'] / 0.00150265118 + 497.06,8)\n",
    "    df['purchase_amount'] = df.purchase_amount.apply(lambda x: np.round(x))\n",
    "    \n",
    "    df['purchase_date']          =  pd.to_datetime(df['purchase_date']) \n",
    "    df['first_active_month']     =  pd.to_datetime(df['first_active_month']) \n",
    "    df['purchase_hour']          =  df['purchase_date'].dt.hour\n",
    "    df['year']                   = df['purchase_date'].dt.year\n",
    "    df['month']                  =  df['purchase_date'].dt.month\n",
    "    df['day']                    = df['purchase_date'].dt.day\n",
    "    df['hour']                   = df['purchase_date'].dt.hour\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['dayofweek']              =  df['purchase_date'].dt.dayofweek\n",
    "    df['weekend']                =  (df.purchase_date.dt.weekday >=5).astype(int) \n",
    "    df                           =  df.sort_values(['card_id','purchase_date']) \n",
    "    df['purchase_date_floorday'] =  df['purchase_date'].dt.floor('d')  #删除小于day的时间\n",
    "    \n",
    "    # 距离激活时间的相对时间,0, 1,2,3,...,max-act\n",
    "    df['purchase_day_since_active_day']   = df['purchase_date_floorday'] - df['first_active_month']  #ht_card_id_gp['purchase_date_floorday'].transform('min')\n",
    "    df['purchase_day_since_active_day']   = df['purchase_day_since_active_day'].dt.days  #.astype('timedelta64[D]') \n",
    "    df['purchase_month_since_active_day'] = df['purchase_day_since_active_day'].agg(month_trans).values\n",
    "    df['purchase_week_since_active_day']  = df['purchase_day_since_active_day'].agg(week_trans).values\n",
    "    \n",
    "    # 距离最后一天时间的相对时间,0,1,2,3,...,max-act\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    df['purchase_day_since_reference_day']   =  ht_card_id_gp['purchase_date_floorday'].transform('max') - df['purchase_date_floorday']\n",
    "    df['purchase_day_since_reference_day']   =  df['purchase_day_since_reference_day'].dt.days\n",
    "    # 一个粗粒度的特征(距离最近购买过去了几周，几月)\n",
    "    df['purchase_week_since_reference_day']  = df['purchase_day_since_reference_day'].agg(week_trans).values\n",
    "    df['purchase_month_since_reference_day'] = df['purchase_day_since_reference_day'].agg(month_trans).values\n",
    "    \n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].shift()\n",
    "    df['purchase_day_diff']   =  df['purchase_date_floorday'].values - df['purchase_day_diff'].values\n",
    "    df['purchase_day_diff']   =  df['purchase_day_diff'].dt.days\n",
    "    df['purchase_week_diff']  =  df['purchase_day_diff'].agg(week_trans).values\n",
    "    df['purchase_month_diff'] =  df['purchase_day_diff'].agg(month_trans).values \n",
    "    \n",
    "    df['purchase_amount_ddgd_98']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.98**x).values\n",
    "    df['purchase_amount_ddgd_99']  = df['purchase_amount'].values * df['purchase_day_since_reference_day'].apply(lambda x:0.99**x).values    \n",
    "    df['purchase_amount_wdgd_96']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.96**x).values \n",
    "    df['purchase_amount_wdgd_97']  = df['purchase_amount'].values * df['purchase_week_since_reference_day'].apply(lambda x:0.97**x).values \n",
    "    df['purchase_amount_mdgd_90']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.9**x).values\n",
    "    df['purchase_amount_mdgd_80']  = df['purchase_amount'].values * df['purchase_month_since_reference_day'].apply(lambda x:0.8**x).values \n",
    "    \n",
    "    df = reduce_mem_usage(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "historical_transactions = get_expand_common(historical_transactions)\n",
    "new_transactions        = get_expand_common(new_transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征优化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate statistics features...\n",
      "generate statistics features...\n",
      "generate statistics features...\n",
      "CPU times: user 5min 51s, sys: 22.6 s, total: 6min 14s\n",
      "Wall time: 6min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 构造基本统计特征\n",
    "def aggregate_transactions(df_, prefix): \n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    df['price'] = df['purchase_amount'] / df['installments']\n",
    "    df['duration'] = df['purchase_amount'] * df['month_diff']\n",
    "    df['amount_month_ratio'] = df['purchase_amount'] / df['month_diff']\n",
    "    \n",
    "    df.loc[:, 'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).\\\n",
    "                                      astype(np.int64) * 1e-9\n",
    "    \n",
    "    agg_func = {\n",
    "        'category_1':      ['mean'],\n",
    "        'category_2':      ['mean'],\n",
    "        'category_3':      ['mean'],\n",
    "        'installments':    ['mean', 'max', 'min', 'std'],\n",
    "        'month_lag':       ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'month':           ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'hour':            ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'weekofyear':      ['nunique', 'mean', 'max', 'min', 'std'],\n",
    "        'dayofweek':       ['nunique', 'mean'],\n",
    "        'weekend':         ['mean'],\n",
    "        'year':            ['nunique'],\n",
    "        'card_id':         ['size','count'],\n",
    "        'purchase_date':   ['max', 'min'],\n",
    "        ###\n",
    "        'price':             ['mean','max','min','std'],\n",
    "        'duration':          ['mean','min','max','std','skew'],\n",
    "        'amount_month_ratio':['mean','min','max','std','skew'],\n",
    "        } \n",
    "    \n",
    "    for col in ['category_2','category_3']:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        agg_func[col+'_mean'] = ['mean']\n",
    "    \n",
    "    agg_df = df.groupby(['card_id']).agg(agg_func)\n",
    "    agg_df.columns = [prefix + '_'.join(col).strip() for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(drop=False, inplace=True)\n",
    "  \n",
    "    return agg_df\n",
    "print('generate statistics features...')\n",
    "auth_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==1], prefix='auth_')\n",
    "print('generate statistics features...')\n",
    "hist_base_stat = aggregate_transactions(historical_transactions[historical_transactions['authorized_flag']==0], prefix='hist_')\n",
    "print('generate statistics features...')\n",
    "new_base_stat  = aggregate_transactions(new_transactions, prefix='new_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auth...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "reference_day 不存在！！！\n",
      "first_day 不存在！！！\n",
      "last_day 不存在！！！\n",
      "activation_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d246de8809400f8563200572b5b798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe59d159d2d442389930771878c8375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d402b338d24d868cca53c9caccceb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mem. usage decreased to 112.08 Mb (73.3% reduction)\n",
      "hist...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      "card_id(month_lag, min to reference day):min\n",
      "last_day 不存在！！！\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcf19f6f2b8487b8370820ddba80841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9485dc6e4f684151a1a52d703121e461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31863832f8c04f379be018e79377f4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mem. usage decreased to 121.08 Mb (72.8% reduction)\n",
      "new...\n",
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " Eight time features, \n",
      "card_id(month_lag, min to reference day):min\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011f197acfb94663bfd1f8e8cab19fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89671ddb71804b9aa9b31106053f9e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5426e9fd569e403d956e130623a644a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************** Part2， data with time less than activation day ******************************\n",
      "card_id(purchase_amount): sum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a3d89f9fd4961b73f9548dee974c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mem. usage decreased to 92.65 Mb (73.4% reduction)\n",
      "CPU times: user 8min 58s, sys: 1min 6s, total: 10min 5s\n",
      "Wall time: 10min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_quantile(x, percentiles = [0.1, 0.25, 0.75, 0.9]):\n",
    "    x_len = len(x)\n",
    "    x = np.sort(x)\n",
    "    sts_feas = []  \n",
    "    for per_ in percentiles:\n",
    "        if per_ == 1:\n",
    "            sts_feas.append(x[x_len - 1]) \n",
    "        else:\n",
    "            sts_feas.append(x[int(x_len * per_)]) \n",
    "    return sts_feas \n",
    "\n",
    "def get_cardf_tran(df_, month = 3, prefix = '_'):\n",
    "    \n",
    "    df = df_.copy() \n",
    "    if prefix == 'hist_cardf_':\n",
    "        df['month_to_now']  =  (datetime.date(2018, month, 1) - df['purchase_date_floorday'].dt.date).dt.days\n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id') \n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    if  prefix == 'hist_cardf_':\n",
    "        cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values\n",
    "        cardid_features['card_id_isau_sum'] = ht_card_id_gp['authorized_flag'].sum().values \n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "    cardid_features['month_diff_median'] = ht_card_id_gp['month_diff'].median().values\n",
    "    \n",
    "    if prefix == 'hist_cardf_':\n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "       \n",
    "        # first to activation day\n",
    "        cardid_features['first_to_activation_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to reference day \n",
    "        cardid_features['activation_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['activation_day']).dt.days\n",
    "        # first to last day \n",
    "        cardid_features['first_to_reference_day']  =  (cardid_features['reference_day'] - cardid_features['first_day']).dt.days\n",
    "        # reference day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_min'] = ht_card_id_gp['month_lag'].agg('min').values   \n",
    "        # is_purchase_before_activation,first_to_reference_day_divide_activation_to_reference_day\n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['first_to_activation_day'] < 0 \n",
    "        cardid_features['is_purchase_before_activation'] = cardid_features['is_purchase_before_activation'].astype(int)\n",
    "        cardid_features['first_to_reference_day_divide_activation_to_reference_day'] = cardid_features['first_to_reference_day']  / (cardid_features['activation_to_reference_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_reference_day'].values / cardid_features['card_id_cnt'].values\n",
    "   \n",
    "    if prefix == 'new_cardf_':\n",
    "        print(' Eight time features, ') \n",
    "        cardid_features['reference_day']           =  ht_card_id_gp['reference_day'].last().values\n",
    "        cardid_features['first_day']               =  ht_card_id_gp['purchase_date_floorday'].min().values \n",
    "        cardid_features['last_day']                =  ht_card_id_gp['purchase_date_floorday'].max().values\n",
    "        cardid_features['activation_day']          =  ht_card_id_gp['first_active_month'].max().values\n",
    "        # reference to first day\n",
    "        cardid_features['reference_day_to_first_day']  =  (cardid_features['first_day'] - cardid_features['reference_day']).dt.days\n",
    "        # reference to last day\n",
    "        cardid_features['reference_day_to_last_day']  =  (cardid_features['last_day'] - cardid_features['reference_day']).dt.days  \n",
    "        # first to last day \n",
    "        cardid_features['first_to_last_day']  =  (cardid_features['last_day'] - cardid_features['first_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_first_day']  =  (cardid_features['first_day'] - cardid_features['activation_day']).dt.days\n",
    "        # activation to first day \n",
    "        cardid_features['activation_to_last_day']  =  (cardid_features['last_day'] - cardid_features['activation_day']).dt.days\n",
    "        # last day to now  \n",
    "        cardid_features['reference_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['reference_day'].dt.date).dt.days \n",
    "        # first day to now\n",
    "        cardid_features['first_day_to_now']  =  (datetime.date(2018, month, 1) - cardid_features['first_day'].dt.date).dt.days \n",
    "        \n",
    "        print('card_id(month_lag, min to reference day):min')\n",
    "        cardid_features['card_id_month_lag_max'] = ht_card_id_gp['month_lag'].agg('max').values  \n",
    "        cardid_features['first_to_last_day_divide_reference_to_last_day'] = cardid_features['first_to_last_day']  / (cardid_features['reference_day_to_last_day']  + 0.01)\n",
    "        cardid_features['days_per_count'] = cardid_features['first_to_last_day'].values / cardid_features['card_id_cnt'].values\n",
    "    \n",
    "    for f in ['reference_day', 'first_day', 'last_day', 'activation_day']:\n",
    "        try:\n",
    "            del cardid_features[f]\n",
    "        except:\n",
    "            print(f, '不存在！！！')\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['category_1','category_2','category_3','state_id','city_id','installments','merchant_id', 'merchant_category_id','subsector_id','month_lag','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col]            =  ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] =  cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    print('card_id(purchase_amount & degrade version ):mean,sum,std,median,quantile(10,25,75,90)') \n",
    "    for col in tqdm_notebook(['installments','purchase_amount','purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median','max','min']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values\n",
    "            \n",
    "            cardid_features['card_id_' +col+ '_range'] =  cardid_features['card_id_' +col+ '_max'].values - cardid_features['card_id_' +col+ '_min'].values\n",
    "            percentiles = ht_card_id_gp[col].apply(lambda x:get_quantile(x,percentiles = [0.025, 0.25, 0.75, 0.975])) \n",
    "\n",
    "            cardid_features[col + '_2.5_quantile']  = percentiles.map(lambda x:x[0]).values\n",
    "            cardid_features[col + '_25_quantile'] = percentiles.map(lambda x:x[1]).values\n",
    "            cardid_features[col + '_75_quantile'] = percentiles.map(lambda x:x[2]).values\n",
    "            cardid_features[col + '_97.5_quantile'] = percentiles.map(lambda x:x[3]).values\n",
    "            cardid_features['card_id_' +col+ '_range2'] =  cardid_features[col+ '_97.5_quantile'].values - cardid_features[col+ '_2.5_quantile'].values\n",
    "            del cardid_features[col + '_2.5_quantile'],cardid_features[col + '_97.5_quantile']\n",
    "            gc.collect()\n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values          \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大installments\n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id',,\n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "    \n",
    "    if prefix == 'new_cardf_':\n",
    "    ######## 在卡未激活之前就有过消费的记录  ##############   \n",
    "        print('*'*30,'Part2， data with time less than activation day','*'*30)\n",
    "        df_part = df.loc[df.purchase_date < df.first_active_month]\n",
    "\n",
    "        cardid_features_part = pd.DataFrame()\n",
    "        cardid_features_part['card_id'] = df_part['card_id'].unique()   \n",
    "        ht_card_id_part_gp = df_part.groupby('card_id')\n",
    "        cardid_features_part['card_id_part_cnt'] = ht_card_id_part_gp['authorized_flag'].count().values\n",
    "\n",
    "        print('card_id(purchase_amount): sum') \n",
    "        for col in tqdm_notebook(['purchase_amount']): \n",
    "            for opt in ['sum','mean']:\n",
    "                cardid_features_part['card_id_part_' +col+ '_' + opt] = ht_card_id_part_gp[col].agg(opt).values\n",
    "\n",
    "        cardid_features = cardid_features.merge(cardid_features_part, on ='card_id', how='left')\n",
    "        cardid_features['card_id_part_purchase_amount_sum_percent'] = cardid_features['card_id_part_purchase_amount_sum'] / (cardid_features['card_id_purchase_amount_sum'] + 0.01)\n",
    "\n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features\n",
    "print('auth...')\n",
    "authorized_transactions = historical_transactions.loc[historical_transactions['authorized_flag'] == 1]\n",
    "auth_cardf_tran = get_cardf_tran(authorized_transactions, 3, prefix='auth_cardf_')\n",
    "print('hist...')\n",
    "hist_cardf_tran = get_cardf_tran(historical_transactions, 3, prefix='hist_cardf_')\n",
    "print('new...')\n",
    "reference_days = historical_transactions.groupby('card_id')['purchase_date'].last().to_frame('reference_day')\n",
    "reference_days.reset_index(inplace = True)\n",
    "new_transactions = new_transactions.merge(reference_days, on ='card_id', how='left')\n",
    "new_cardf_tran  = get_cardf_tran(new_transactions, 5, prefix='new_cardf_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** Part1, whole data ******************************\n",
      "****************************** Traditional Features ******************************\n",
      " card id : count\n",
      "card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45172045cce54a9f939a33b07975e437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975cdb6622634db6811334cf9b50b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************** Pivot Features ******************************\n",
      "Count  Pivot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc1826c7dff402091a5619f80beb24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mem. usage decreased to 84.45 Mb (74.0% reduction)\n",
      "CPU times: user 1min 26s, sys: 3.63 s, total: 1min 30s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_cardf_tran_last2(df_, month = 3, prefix = 'last2_'): \n",
    "    \n",
    "    df = df_.loc[df_.month_lag >= -2].copy()\n",
    "    print('*'*30,'Part1, whole data','*'*30)\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()   \n",
    "    \n",
    "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] = df['month_diff'].astype(int)\n",
    "    df['month_diff'] += df['month_lag']\n",
    "    \n",
    "    print( '*' * 30, 'Traditional Features', '*' * 30)\n",
    "    ht_card_id_gp = df.groupby('card_id')\n",
    "    print(' card id : count')\n",
    "    cardid_features['card_id_cnt'] = ht_card_id_gp['authorized_flag'].count().values\n",
    "    \n",
    "    cardid_features['card_id_isau_mean'] = ht_card_id_gp['authorized_flag'].mean().values \n",
    "    cardid_features['card_id_isau_sum']  = ht_card_id_gp['authorized_flag'].sum().values\n",
    "    \n",
    "    cardid_features['month_diff_mean']   = ht_card_id_gp['month_diff'].mean().values\n",
    "\n",
    "    print('card id(city_id,installments,merchant_category_id,.......):nunique, cnt/nunique') \n",
    "    for col in tqdm_notebook(['state_id','city_id','installments','merchant_id', 'merchant_category_id','purchase_date_floorday']):\n",
    "        cardid_features['card_id_%s_nunique'%col] = ht_card_id_gp[col].nunique().values\n",
    "        cardid_features['card_id_cnt_divide_%s_nunique'%col] = cardid_features['card_id_cnt'].values / cardid_features['card_id_%s_nunique'%col].values\n",
    "         \n",
    "    for col in tqdm_notebook(['purchase_amount','purchase_amount_ddgd_98','purchase_amount_wdgd_96','purchase_amount_mdgd_90','purchase_amount_mdgd_80']): #,'purchase_amount_ddgd_98','purchase_amount_ddgd_99','purchase_amount_wdgd_96','purchase_amount_wdgd_97','purchase_amount_mdgd_90','purchase_amount_mdgd_80']):\n",
    "        if col =='purchase_amount':\n",
    "            for opt in ['sum','mean','std','median']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values  \n",
    "        else:\n",
    "            for opt in ['sum']:\n",
    "                cardid_features['card_id_' +col+ '_' + opt] = ht_card_id_gp[col].agg(opt).values \n",
    "    \n",
    "    print( '*' * 30, 'Pivot Features', '*' * 30)\n",
    "    print('Count  Pivot') #purchase_month_since_reference_day(可能和month_lag重复),百分比降分,暂时忽略 (dayofweek,merchant_cate,state_id)作用不大\n",
    "    \n",
    "    for pivot_col in tqdm_notebook(['category_1','category_2','category_3','month_lag','subsector_id','weekend']): #'city_id', \n",
    "    \n",
    "        tmp     = df.groupby(['card_id',pivot_col])['merchant_id'].count().to_frame(pivot_col + '_count')\n",
    "        tmp.reset_index(inplace =True)  \n",
    "         \n",
    "        tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_count',fill_value=0)\n",
    "        tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_cnt_pivot_'+ str(col) for col in tmp_pivot.columns]\n",
    "        tmp_pivot.reset_index(inplace = True)\n",
    "        cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "      \n",
    "        if  pivot_col!='weekend' and  pivot_col!='installments':\n",
    "            tmp            = df.groupby(['card_id',pivot_col])['purchase_date_floorday'].nunique().to_frame(pivot_col + '_purchase_date_floorday_nunique') \n",
    "            tmp1           = df.groupby(['card_id'])['purchase_date_floorday'].nunique().to_frame('purchase_date_floorday_nunique') \n",
    "            tmp.reset_index(inplace =True)  \n",
    "            tmp1.reset_index(inplace =True)   \n",
    "            tmp  = tmp.merge(tmp1, on ='card_id', how='left')\n",
    "            tmp[pivot_col + '_day_nunique_pct'] = tmp[pivot_col + '_purchase_date_floorday_nunique'].values / tmp['purchase_date_floorday_nunique'].values\n",
    "         \n",
    "            tmp_pivot = pd.pivot_table(data=tmp,index = 'card_id',columns=pivot_col,values=pivot_col + '_day_nunique_pct',fill_value=0)\n",
    "            tmp_pivot.columns = [tmp_pivot.columns.names[0] + '_day_nunique_pct_'+ str(col) for col in tmp_pivot.columns]\n",
    "            tmp_pivot.reset_index(inplace = True)\n",
    "            cardid_features = cardid_features.merge(tmp_pivot, on = 'card_id', how='left')\n",
    "     \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "hist_cardf_tran_last2 = get_cardf_tran_last2(historical_transactions, month = 3, prefix = 'hist_last2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ac7ba9e3ad4916bbce5e2b1f5bcfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3fbe1cee7e4676a2365136a202442f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7724b12e394eeca501915868b8f8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mem. usage decreased to 70.16 Mb (66.8% reduction)\n",
      "CPU times: user 5min 5s, sys: 58.7 s, total: 6min 3s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def successive_aggregates(df_, prefix = 'levelAB_'):\n",
    "    df = df_.copy()\n",
    "    cardid_features = pd.DataFrame()\n",
    "    cardid_features['card_id'] = df['card_id'].unique()    \n",
    "     \n",
    "    level12_nunique = [('month_lag','state_id'),('month_lag','city_id'),('month_lag','subsector_id'),('month_lag','merchant_category_id'),('month_lag','merchant_id'),('month_lag','purchase_date_floorday'),\\\n",
    "                       ('subsector_id','merchant_category_id'),('subsector_id','merchant_id'),('subsector_id','purchase_date_floorday'),('subsector_id','month_lag'),\\\n",
    "                       ('merchant_category_id', 'merchant_id'),('merchant_category_id','purchase_date_floorday'),('merchant_category_id','month_lag'),\\\n",
    "                       ('purchase_date_floorday', 'merchant_id'),('purchase_date_floorday','merchant_category_id'),('purchase_date_floorday','subsector_id')]    \n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_nunique):  \n",
    "        \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].nunique().to_frame(col_level2 + '_nunique')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_nunique'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_nunique_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_count = ['month_lag','state_id','city_id','subsector_id','merchant_category_id','merchant_id','purchase_date_floorday']\n",
    "    for col_level in tqdm_notebook(level12_count): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level])['merchant_id'].count().to_frame(col_level + '_count')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level + '_count'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level + '_count_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "        \n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left') \n",
    "    \n",
    "    level12_meansum = [('month_lag','purchase_amount'),('state_id','purchase_amount'),('city_id','purchase_amount'),('subsector_id','purchase_amount'),\\\n",
    "                       ('merchant_category_id','purchase_amount'),('merchant_id','purchase_amount'),('purchase_date_floorday','purchase_amount')]\n",
    "    for col_level1,col_level2 in tqdm_notebook(level12_meansum): \n",
    "    \n",
    "        level1  = df.groupby(['card_id',col_level1])[col_level2].sum().to_frame(col_level2 + '_sum')\n",
    "        level1.reset_index(inplace =True)  \n",
    "         \n",
    "        level2 = level1.groupby('card_id')[col_level2 + '_sum'].agg(['mean', 'max', 'std'])\n",
    "        level2 = pd.DataFrame(level2)\n",
    "        level2.columns = [col_level1 + '_' + col_level2 + '_sum_' + col for col in level2.columns.values]\n",
    "        level2.reset_index(inplace = True)\n",
    "\n",
    "        cardid_features = cardid_features.merge(level2, on='card_id', how='left')           \n",
    "    \n",
    "    cardid_features = reduce_mem_usage(cardid_features)\n",
    "    \n",
    "    new_col_names = []\n",
    "    for col in cardid_features.columns:\n",
    "        if col == 'card_id':\n",
    "            new_col_names.append(col)\n",
    "        else:\n",
    "            new_col_names.append(prefix + col)\n",
    "    cardid_features.columns = new_col_names\n",
    "    \n",
    "    return cardid_features  \n",
    "\n",
    "print('hist...')\n",
    "hist_levelAB = successive_aggregates(historical_transactions, prefix = 'hist_levelAB_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201917, 11)\n",
      "(123623, 10)\n",
      "#_____基础统计特征\n",
      "(201917, 164)\n",
      "(123623, 163)\n",
      "#_____全局cardid特征\n",
      "(201917, 687)\n",
      "(123623, 686)\n",
      "#_____最近两月cardid特征\n",
      "(201917, 821)\n",
      "(123623, 820)\n",
      "#_____补充二阶特征\n",
      "(201917, 911)\n",
      "(123623, 910)\n",
      "CPU times: user 41.3 s, sys: 2 s, total: 43.3 s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "## 合并到训练集和测试集\n",
    "print('#_____基础统计特征')\n",
    "train = pd.merge(train, auth_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_base_stat, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_base_stat, on='card_id', how='left')\n",
    "train = pd.merge(train, new_base_stat , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_base_stat , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____全局cardid特征')\n",
    "train = pd.merge(train, auth_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  auth_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, hist_cardf_tran, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran, on='card_id', how='left')\n",
    "train = pd.merge(train, new_cardf_tran , on='card_id', how='left')\n",
    "test  = pd.merge(test,  new_cardf_tran , on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____最近两月cardid特征')\n",
    "train = pd.merge(train, hist_cardf_tran_last2, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_cardf_tran_last2, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print('#_____补充二阶特征')\n",
    "train = pd.merge(train, hist_levelAB, on='card_id', how='left')\n",
    "test  = pd.merge(test,  hist_levelAB, on='card_id', how='left')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 562 ms, sys: 796 ms, total: 1.36 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train['outliers'] = 0\n",
    "train.loc[train['target'] < -30, 'outliers'] = 1\n",
    "train['outliers'].value_counts()\n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    colname = f+'_outliers_mean'\n",
    "    order_label = train.groupby([f])['outliers'].mean()\n",
    "    for df in [train, test]:\n",
    "        df[colname] = df[f].map(order_label)\n",
    "\n",
    "for df in [train, test]:\n",
    "    \n",
    "    df['days_feature1'] = df['elapsed_time'] * df['feature_1']\n",
    "    df['days_feature2'] = df['elapsed_time'] * df['feature_2']\n",
    "    df['days_feature3'] = df['elapsed_time'] * df['feature_3']\n",
    "\n",
    "    df['days_feature1_ratio'] = df['feature_1'] / df['elapsed_time']\n",
    "    df['days_feature2_ratio'] = df['feature_2'] / df['elapsed_time']\n",
    "    df['days_feature3_ratio'] = df['feature_3'] / df['elapsed_time']\n",
    "\n",
    "    df['feature_sum'] = df['feature_1'] + df['feature_2'] + df['feature_3']\n",
    "    df['feature_mean'] = df['feature_sum']/3\n",
    "    df['feature_max'] = df[['feature_1', 'feature_2', 'feature_3']].max(axis=1)\n",
    "    df['feature_min'] = df[['feature_1', 'feature_2', 'feature_3']].min(axis=1)\n",
    "    df['feature_var'] = df[['feature_1', 'feature_2', 'feature_3']].std(axis=1)\n",
    "    \n",
    "    df['card_id_total'] = df['hist_card_id_size']+df['new_card_id_size']\n",
    "    df['card_id_cnt_total'] = df['hist_card_id_count']+df['new_card_id_count']\n",
    "    df['card_id_cnt_ratio'] = df['new_card_id_count']/df['hist_card_id_count']\n",
    "    df['purchase_amount_total'] = df['hist_cardf_card_id_purchase_amount_sum']+df['new_cardf_card_id_purchase_amount_sum']\n",
    "    df['purchase_amount_ratio'] = df['new_cardf_card_id_purchase_amount_sum']/df['hist_cardf_card_id_purchase_amount_sum']\n",
    "    df['month_diff_ratio'] = df['new_cardf_month_diff_mean']/df['hist_cardf_month_diff_mean']\n",
    "    df['installments_total'] = df['new_cardf_card_id_installments_sum']+df['auth_cardf_card_id_installments_sum']\n",
    "    df['installments_ratio'] = df['new_cardf_card_id_installments_sum']/df['auth_cardf_card_id_installments_sum']\n",
    "    df['price_total'] = df['purchase_amount_total']/df['installments_total']\n",
    "    df['new_CLV'] = df['new_card_id_count'] * df['new_cardf_card_id_purchase_amount_sum'] / df['new_cardf_month_diff_mean']\n",
    "    df['hist_CLV'] = df['hist_card_id_count'] * df['hist_cardf_card_id_purchase_amount_sum'] / df['hist_cardf_month_diff_mean']\n",
    "    df['CLV_ratio'] = df['new_CLV'] / df['hist_CLV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征基本过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运用杰哥方法...\n",
      "删除前: 771\n",
      "删除后: 770\n",
      "CPU times: user 3min 4s, sys: 1.45 s, total: 3min 5s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "del_cols = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'new_cardf': \n",
    "        del_cols.append(col)\n",
    "del_cols1 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'hist_last2_' in col:\n",
    "        del_cols1.append(col)\n",
    "del_cols2 = []\n",
    "for col in train.columns:\n",
    "    if 'subsector_id_cnt_' in col and 'auth_cardf' in col:\n",
    "        del_cols2.append(col)\n",
    "del_cols3 = []\n",
    "for col in train.columns:\n",
    "    if 'merchant_category_id_month_lag_nunique_' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'city_id' in col and '_pivot_supp' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff' in col and 'hist_last2_' in col:\n",
    "        del_cols3.append(col)\n",
    "    if 'month_diff_std' in col or 'month_diff_gap' in col:\n",
    "        del_cols3.append(col) \n",
    "fea_cols = [col for col in train.columns if train[col].dtypes!='object' and train[col].dtypes != '<M8[ns]' and col!='target' not in col and col!='min_num'\\\n",
    "            and col not in del_cols and col not in del_cols1 and col not in del_cols2 and col!='target1' and col!='card_id_cnt_ht_pivot_supp'  and col not in del_cols3]   \n",
    "print('运用杰哥方法...')\n",
    "print('删除前:',train.shape[1])\n",
    "print('删除后:',len(fea_cols))\n",
    "\n",
    "train = train[fea_cols+['target']]\n",
    "fea_cols.remove('outliers')\n",
    "test = test[fea_cols]\n",
    "\n",
    "train.to_csv('./data/all_train_features.csv',index=False)\n",
    "test.to_csv('./data/all_test_features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (201917, 771)\n",
      "ntrain: (199710, 771)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## load all features\n",
    "train = pd.read_csv('./data/all_train_features.csv')\n",
    "test  = pd.read_csv('./data/all_test_features.csv')\n",
    "\n",
    "# ## load sparse\n",
    "# train_tags = sparse.load_npz('train_tags.npz')\n",
    "# test_tags  = sparse.load_npz('test_tags.npz')\n",
    "\n",
    "## 获取非异常值的index\n",
    "normal_index = train[train['outliers']==0].index.tolist()\n",
    "## without outliers\n",
    "ntrain = train[train['outliers'] == 0]\n",
    "\n",
    "target        = train['target'].values\n",
    "ntarget       = ntrain['target'].values\n",
    "target_binary = train['outliers'].values\n",
    "###\n",
    "y_train        = target\n",
    "y_ntrain       = ntarget\n",
    "y_train_binary = target_binary\n",
    "\n",
    "print('train:',train.shape)\n",
    "print('ntrain:',ntrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', eval_type='regression'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params, trn_data, num_boost_round=20000, \n",
    "                            valid_sets=[trn_data, val_data], \n",
    "                            verbose_eval=100, early_stopping_rounds=300)\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            trn_data = xgb.DMatrix(X[trn_idx], y[trn_idx])\n",
    "            val_data = xgb.DMatrix(X[val_idx], y[val_idx])\n",
    "            watchlist = [(trn_data, 'train'), (val_data, 'valid_data')]\n",
    "            clf = xgb.train(dtrain=trn_data, num_boost_round=20000, \n",
    "                            evals=watchlist, early_stopping_rounds=200, \n",
    "                            verbose_eval=100, params=params)\n",
    "            oof[val_idx] = clf.predict(xgb.DMatrix(X[val_idx]), ntree_limit=clf.best_ntree_limit)\n",
    "            predictions += clf.predict(xgb.DMatrix(X_test), ntree_limit=clf.best_ntree_limit) / folds.n_splits\n",
    "        \n",
    "        if (model_type == 'cat') and (eval_type == 'regression'):\n",
    "            clf = CatBoostRegressor(iterations=20000, eval_metric='RMSE', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict(X[val_idx])\n",
    "            predictions += clf.predict(X_test) / folds.n_splits\n",
    "            \n",
    "        if (model_type == 'cat') and (eval_type == 'binary'):\n",
    "            clf = CatBoostClassifier(iterations=20000, eval_metric='Logloss', **params)\n",
    "            clf.fit(X[trn_idx], y[trn_idx], \n",
    "                    eval_set=(X[val_idx], y[val_idx]),\n",
    "                    cat_features=[], use_best_model=True, verbose=100)\n",
    "            oof[val_idx] = clf.predict_proba(X[val_idx])[:,1]\n",
    "            predictions += clf.predict_proba(X_test)[:,1] / folds.n_splits\n",
    "        print(predictions)\n",
    "        if eval_type == 'regression':\n",
    "            scores.append(mean_squared_error(oof[val_idx], y[val_idx])**0.5)\n",
    "        if eval_type == 'binary':\n",
    "            scores.append(log_loss(y[val_idx], oof[val_idx]))\n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 08:54:05 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 3.54042\tvalid_1's rmse: 3.78686\n",
      "[200]\ttraining's rmse: 3.39771\tvalid_1's rmse: 3.74807\n",
      "[300]\ttraining's rmse: 3.30497\tvalid_1's rmse: 3.73399\n",
      "[400]\ttraining's rmse: 3.23606\tvalid_1's rmse: 3.72669\n",
      "[500]\ttraining's rmse: 3.1811\tvalid_1's rmse: 3.72438\n",
      "[600]\ttraining's rmse: 3.13103\tvalid_1's rmse: 3.72295\n",
      "[700]\ttraining's rmse: 3.08693\tvalid_1's rmse: 3.72215\n",
      "[800]\ttraining's rmse: 3.04662\tvalid_1's rmse: 3.72162\n",
      "[900]\ttraining's rmse: 3.01022\tvalid_1's rmse: 3.72209\n",
      "[1000]\ttraining's rmse: 2.97563\tvalid_1's rmse: 3.72162\n",
      "[1100]\ttraining's rmse: 2.94287\tvalid_1's rmse: 3.72155\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttraining's rmse: 3.04057\tvalid_1's rmse: 3.72144\n",
      "[-0.42731101 -0.04211712 -0.13554804 ...  0.15400129 -0.64100008\n",
      "  0.04136181]\n",
      "Fold 1 started at Wed Oct 28 08:58:08 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 3.57121\tvalid_1's rmse: 3.65909\n",
      "[200]\ttraining's rmse: 3.42378\tvalid_1's rmse: 3.62186\n",
      "[300]\ttraining's rmse: 3.33175\tvalid_1's rmse: 3.60963\n",
      "[400]\ttraining's rmse: 3.26574\tvalid_1's rmse: 3.60333\n",
      "[500]\ttraining's rmse: 3.21137\tvalid_1's rmse: 3.5995\n",
      "[600]\ttraining's rmse: 3.16326\tvalid_1's rmse: 3.59749\n",
      "[700]\ttraining's rmse: 3.12083\tvalid_1's rmse: 3.59624\n",
      "[800]\ttraining's rmse: 3.08025\tvalid_1's rmse: 3.59568\n",
      "[900]\ttraining's rmse: 3.04293\tvalid_1's rmse: 3.59545\n",
      "[1000]\ttraining's rmse: 3.00903\tvalid_1's rmse: 3.59554\n",
      "[1100]\ttraining's rmse: 2.97404\tvalid_1's rmse: 3.59511\n",
      "[1200]\ttraining's rmse: 2.94307\tvalid_1's rmse: 3.5948\n",
      "[1300]\ttraining's rmse: 2.91314\tvalid_1's rmse: 3.5951\n",
      "[1400]\ttraining's rmse: 2.88229\tvalid_1's rmse: 3.59522\n",
      "[1500]\ttraining's rmse: 2.8545\tvalid_1's rmse: 3.59548\n",
      "Early stopping, best iteration is:\n",
      "[1249]\ttraining's rmse: 2.92824\tvalid_1's rmse: 3.59446\n",
      "[-0.8338159  -0.13369747 -0.28434003 ...  0.32688221 -1.35716394\n",
      "  0.03924934]\n",
      "Fold 2 started at Wed Oct 28 09:03:32 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 3.56331\tvalid_1's rmse: 3.69604\n",
      "[200]\ttraining's rmse: 3.41621\tvalid_1's rmse: 3.66195\n",
      "[300]\ttraining's rmse: 3.32231\tvalid_1's rmse: 3.65071\n",
      "[400]\ttraining's rmse: 3.25426\tvalid_1's rmse: 3.64522\n",
      "[500]\ttraining's rmse: 3.20059\tvalid_1's rmse: 3.64271\n",
      "[600]\ttraining's rmse: 3.1528\tvalid_1's rmse: 3.64121\n",
      "[700]\ttraining's rmse: 3.11025\tvalid_1's rmse: 3.641\n",
      "[800]\ttraining's rmse: 3.07064\tvalid_1's rmse: 3.6405\n",
      "[900]\ttraining's rmse: 3.03506\tvalid_1's rmse: 3.64018\n",
      "[1000]\ttraining's rmse: 3.0004\tvalid_1's rmse: 3.63975\n",
      "[1100]\ttraining's rmse: 2.96736\tvalid_1's rmse: 3.64022\n",
      "[1200]\ttraining's rmse: 2.93613\tvalid_1's rmse: 3.64043\n",
      "Early stopping, best iteration is:\n",
      "[996]\ttraining's rmse: 3.0017\tvalid_1's rmse: 3.6397\n",
      "[-1.11752509 -0.21454436 -0.43619923 ...  0.4961004  -1.91334386\n",
      "  0.06205305]\n",
      "Fold 3 started at Wed Oct 28 09:08:06 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 3.58276\tvalid_1's rmse: 3.59534\n",
      "[200]\ttraining's rmse: 3.43717\tvalid_1's rmse: 3.56569\n",
      "[300]\ttraining's rmse: 3.34536\tvalid_1's rmse: 3.55707\n",
      "[400]\ttraining's rmse: 3.27777\tvalid_1's rmse: 3.55362\n",
      "[500]\ttraining's rmse: 3.22189\tvalid_1's rmse: 3.5529\n",
      "[600]\ttraining's rmse: 3.17364\tvalid_1's rmse: 3.55262\n",
      "[700]\ttraining's rmse: 3.12924\tvalid_1's rmse: 3.55259\n",
      "[800]\ttraining's rmse: 3.08925\tvalid_1's rmse: 3.55276\n",
      "[900]\ttraining's rmse: 3.05221\tvalid_1's rmse: 3.55251\n",
      "[1000]\ttraining's rmse: 3.01808\tvalid_1's rmse: 3.55324\n",
      "Early stopping, best iteration is:\n",
      "[760]\ttraining's rmse: 3.10462\tvalid_1's rmse: 3.55236\n",
      "[-1.48686402 -0.26620603 -0.58434079 ...  0.66313937 -2.57906612\n",
      "  0.10060466]\n",
      "Fold 4 started at Wed Oct 28 09:11:59 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 3.54712\tvalid_1's rmse: 3.76054\n",
      "[200]\ttraining's rmse: 3.40342\tvalid_1's rmse: 3.72344\n",
      "[300]\ttraining's rmse: 3.30994\tvalid_1's rmse: 3.70958\n",
      "[400]\ttraining's rmse: 3.24328\tvalid_1's rmse: 3.70539\n",
      "[500]\ttraining's rmse: 3.18746\tvalid_1's rmse: 3.70274\n",
      "[600]\ttraining's rmse: 3.14051\tvalid_1's rmse: 3.70166\n",
      "[700]\ttraining's rmse: 3.09714\tvalid_1's rmse: 3.70144\n",
      "[800]\ttraining's rmse: 3.05729\tvalid_1's rmse: 3.7015\n",
      "[900]\ttraining's rmse: 3.02181\tvalid_1's rmse: 3.70186\n",
      "Early stopping, best iteration is:\n",
      "[680]\ttraining's rmse: 3.10508\tvalid_1's rmse: 3.70117\n",
      "[-1.99563085 -0.34504237 -0.75340554 ...  0.78328748 -3.23609847\n",
      "  0.13522232]\n",
      "CV mean score: 3.6418, std: 0.0634.\n"
     ]
    }
   ],
   "source": [
    "#### lgb\n",
    "lgb_params = {'num_leaves': 63,\n",
    "             'min_data_in_leaf': 32, \n",
    "             'objective':'regression',\n",
    "             'max_depth': -1,\n",
    "             'learning_rate': 0.01,\n",
    "             \"min_child_samples\": 20,\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"feature_fraction\": 0.9,\n",
    "             \"bagging_freq\": 1,\n",
    "             \"bagging_fraction\": 0.9 ,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             \"lambda_l1\": 0.1,\n",
    "             \"verbosity\": -1}\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=4096)\n",
    "X_ntrain = ntrain[fea_cols].values\n",
    "X_train  = train[fea_cols].values\n",
    "X_test   = test[fea_cols].values\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_lgb , predictions_lgb , scores_lgb  = train_model(X_train , X_test, y_train, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 09:15:38 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 1.58513\tvalid_1's rmse: 1.57343\n",
      "[200]\ttraining's rmse: 1.5445\tvalid_1's rmse: 1.5478\n",
      "[300]\ttraining's rmse: 1.52304\tvalid_1's rmse: 1.54022\n",
      "[400]\ttraining's rmse: 1.50747\tvalid_1's rmse: 1.53692\n",
      "[500]\ttraining's rmse: 1.49423\tvalid_1's rmse: 1.53505\n",
      "[600]\ttraining's rmse: 1.48227\tvalid_1's rmse: 1.53403\n",
      "[700]\ttraining's rmse: 1.47138\tvalid_1's rmse: 1.53348\n",
      "[800]\ttraining's rmse: 1.46138\tvalid_1's rmse: 1.53322\n",
      "[900]\ttraining's rmse: 1.45164\tvalid_1's rmse: 1.53315\n",
      "[1000]\ttraining's rmse: 1.44246\tvalid_1's rmse: 1.53304\n",
      "[1100]\ttraining's rmse: 1.43359\tvalid_1's rmse: 1.53295\n",
      "[1200]\ttraining's rmse: 1.42489\tvalid_1's rmse: 1.53295\n",
      "[1300]\ttraining's rmse: 1.4164\tvalid_1's rmse: 1.53277\n",
      "[1400]\ttraining's rmse: 1.40822\tvalid_1's rmse: 1.53274\n",
      "[1500]\ttraining's rmse: 1.40016\tvalid_1's rmse: 1.53276\n",
      "[1600]\ttraining's rmse: 1.39232\tvalid_1's rmse: 1.53277\n",
      "[1700]\ttraining's rmse: 1.38425\tvalid_1's rmse: 1.53274\n",
      "[1800]\ttraining's rmse: 1.37638\tvalid_1's rmse: 1.53276\n",
      "[1900]\ttraining's rmse: 1.3688\tvalid_1's rmse: 1.53267\n",
      "[2000]\ttraining's rmse: 1.36147\tvalid_1's rmse: 1.53279\n",
      "[2100]\ttraining's rmse: 1.35406\tvalid_1's rmse: 1.53288\n",
      "Early stopping, best iteration is:\n",
      "[1880]\ttraining's rmse: 1.37031\tvalid_1's rmse: 1.53265\n",
      "[-0.07848529 -0.06718209 -0.08530607 ...  0.19759297 -0.1277005\n",
      "  0.01307935]\n",
      "Fold 1 started at Wed Oct 28 09:23:33 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 1.57477\tvalid_1's rmse: 1.62083\n",
      "[200]\ttraining's rmse: 1.53486\tvalid_1's rmse: 1.59165\n",
      "[300]\ttraining's rmse: 1.51377\tvalid_1's rmse: 1.58213\n",
      "[400]\ttraining's rmse: 1.49831\tvalid_1's rmse: 1.57815\n",
      "[500]\ttraining's rmse: 1.48513\tvalid_1's rmse: 1.57633\n",
      "[600]\ttraining's rmse: 1.47324\tvalid_1's rmse: 1.57543\n",
      "[700]\ttraining's rmse: 1.46246\tvalid_1's rmse: 1.57489\n",
      "[800]\ttraining's rmse: 1.45232\tvalid_1's rmse: 1.57441\n",
      "[900]\ttraining's rmse: 1.44279\tvalid_1's rmse: 1.57445\n",
      "[1000]\ttraining's rmse: 1.43356\tvalid_1's rmse: 1.57427\n",
      "[1100]\ttraining's rmse: 1.42471\tvalid_1's rmse: 1.57419\n",
      "[1200]\ttraining's rmse: 1.41619\tvalid_1's rmse: 1.57401\n",
      "[1300]\ttraining's rmse: 1.40761\tvalid_1's rmse: 1.57392\n",
      "[1400]\ttraining's rmse: 1.39944\tvalid_1's rmse: 1.57377\n",
      "[1500]\ttraining's rmse: 1.39131\tvalid_1's rmse: 1.57374\n",
      "[1600]\ttraining's rmse: 1.38338\tvalid_1's rmse: 1.57361\n",
      "[1700]\ttraining's rmse: 1.37563\tvalid_1's rmse: 1.57365\n",
      "[1800]\ttraining's rmse: 1.36784\tvalid_1's rmse: 1.57357\n",
      "[1900]\ttraining's rmse: 1.36018\tvalid_1's rmse: 1.57363\n",
      "[2000]\ttraining's rmse: 1.35259\tvalid_1's rmse: 1.57368\n",
      "[2100]\ttraining's rmse: 1.34517\tvalid_1's rmse: 1.57358\n",
      "Early stopping, best iteration is:\n",
      "[1815]\ttraining's rmse: 1.36669\tvalid_1's rmse: 1.57355\n",
      "[-0.12924909 -0.09671792 -0.17980471 ...  0.37334976 -0.23791025\n",
      "  0.04020927]\n",
      "Fold 2 started at Wed Oct 28 09:31:25 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 1.58262\tvalid_1's rmse: 1.58702\n",
      "[200]\ttraining's rmse: 1.54269\tvalid_1's rmse: 1.55981\n",
      "[300]\ttraining's rmse: 1.52164\tvalid_1's rmse: 1.55122\n",
      "[400]\ttraining's rmse: 1.50606\tvalid_1's rmse: 1.54751\n",
      "[500]\ttraining's rmse: 1.49289\tvalid_1's rmse: 1.54546\n",
      "[600]\ttraining's rmse: 1.48109\tvalid_1's rmse: 1.5446\n",
      "[700]\ttraining's rmse: 1.47046\tvalid_1's rmse: 1.54407\n",
      "[800]\ttraining's rmse: 1.46041\tvalid_1's rmse: 1.54379\n",
      "[900]\ttraining's rmse: 1.4507\tvalid_1's rmse: 1.54361\n",
      "[1000]\ttraining's rmse: 1.44156\tvalid_1's rmse: 1.54336\n",
      "[1100]\ttraining's rmse: 1.43276\tvalid_1's rmse: 1.54328\n",
      "[1200]\ttraining's rmse: 1.42432\tvalid_1's rmse: 1.54319\n",
      "[1300]\ttraining's rmse: 1.41589\tvalid_1's rmse: 1.54316\n",
      "[1400]\ttraining's rmse: 1.40771\tvalid_1's rmse: 1.54306\n",
      "[1500]\ttraining's rmse: 1.39948\tvalid_1's rmse: 1.54303\n",
      "[1600]\ttraining's rmse: 1.39156\tvalid_1's rmse: 1.54296\n",
      "[1700]\ttraining's rmse: 1.38364\tvalid_1's rmse: 1.54301\n",
      "[1800]\ttraining's rmse: 1.37589\tvalid_1's rmse: 1.54292\n",
      "[1900]\ttraining's rmse: 1.36834\tvalid_1's rmse: 1.54284\n",
      "[2000]\ttraining's rmse: 1.3609\tvalid_1's rmse: 1.54279\n",
      "[2100]\ttraining's rmse: 1.35346\tvalid_1's rmse: 1.54279\n",
      "[2200]\ttraining's rmse: 1.3462\tvalid_1's rmse: 1.54283\n",
      "[2300]\ttraining's rmse: 1.33901\tvalid_1's rmse: 1.54299\n",
      "Early stopping, best iteration is:\n",
      "[2026]\ttraining's rmse: 1.35893\tvalid_1's rmse: 1.54271\n",
      "[-0.20735034 -0.1526776  -0.29194122 ...  0.54775664 -0.36920385\n",
      "  0.06110374]\n",
      "Fold 3 started at Wed Oct 28 09:39:38 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 1.57968\tvalid_1's rmse: 1.59444\n",
      "[200]\ttraining's rmse: 1.53925\tvalid_1's rmse: 1.56863\n",
      "[300]\ttraining's rmse: 1.5181\tvalid_1's rmse: 1.5609\n",
      "[400]\ttraining's rmse: 1.50246\tvalid_1's rmse: 1.55747\n",
      "[500]\ttraining's rmse: 1.48904\tvalid_1's rmse: 1.55568\n",
      "[600]\ttraining's rmse: 1.47711\tvalid_1's rmse: 1.55473\n",
      "[700]\ttraining's rmse: 1.46612\tvalid_1's rmse: 1.55426\n",
      "[800]\ttraining's rmse: 1.45591\tvalid_1's rmse: 1.55391\n",
      "[900]\ttraining's rmse: 1.44634\tvalid_1's rmse: 1.55372\n",
      "[1000]\ttraining's rmse: 1.43704\tvalid_1's rmse: 1.55357\n",
      "[1100]\ttraining's rmse: 1.42814\tvalid_1's rmse: 1.55346\n",
      "[1200]\ttraining's rmse: 1.41963\tvalid_1's rmse: 1.55344\n",
      "[1300]\ttraining's rmse: 1.41135\tvalid_1's rmse: 1.55335\n",
      "[1400]\ttraining's rmse: 1.4029\tvalid_1's rmse: 1.55334\n",
      "[1500]\ttraining's rmse: 1.39476\tvalid_1's rmse: 1.55328\n",
      "[1600]\ttraining's rmse: 1.38663\tvalid_1's rmse: 1.55347\n",
      "[1700]\ttraining's rmse: 1.37877\tvalid_1's rmse: 1.55354\n",
      "Early stopping, best iteration is:\n",
      "[1460]\ttraining's rmse: 1.398\tvalid_1's rmse: 1.55326\n",
      "[-0.26277496 -0.2228768  -0.39709414 ...  0.69005353 -0.49878331\n",
      "  0.10581327]\n",
      "Fold 4 started at Wed Oct 28 09:46:18 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's rmse: 1.58193\tvalid_1's rmse: 1.58904\n",
      "[200]\ttraining's rmse: 1.54154\tvalid_1's rmse: 1.56129\n",
      "[300]\ttraining's rmse: 1.52032\tvalid_1's rmse: 1.55267\n",
      "[400]\ttraining's rmse: 1.50494\tvalid_1's rmse: 1.54883\n",
      "[500]\ttraining's rmse: 1.49163\tvalid_1's rmse: 1.54653\n",
      "[600]\ttraining's rmse: 1.47994\tvalid_1's rmse: 1.54523\n",
      "[700]\ttraining's rmse: 1.46901\tvalid_1's rmse: 1.5444\n",
      "[800]\ttraining's rmse: 1.45887\tvalid_1's rmse: 1.54394\n",
      "[900]\ttraining's rmse: 1.44935\tvalid_1's rmse: 1.54369\n",
      "[1000]\ttraining's rmse: 1.44021\tvalid_1's rmse: 1.54341\n",
      "[1100]\ttraining's rmse: 1.43126\tvalid_1's rmse: 1.54323\n",
      "[1200]\ttraining's rmse: 1.42258\tvalid_1's rmse: 1.54302\n",
      "[1300]\ttraining's rmse: 1.41409\tvalid_1's rmse: 1.54285\n",
      "[1400]\ttraining's rmse: 1.40564\tvalid_1's rmse: 1.54268\n",
      "[1500]\ttraining's rmse: 1.39741\tvalid_1's rmse: 1.54257\n",
      "[1600]\ttraining's rmse: 1.3895\tvalid_1's rmse: 1.54251\n",
      "[1700]\ttraining's rmse: 1.38162\tvalid_1's rmse: 1.54248\n",
      "[1800]\ttraining's rmse: 1.37396\tvalid_1's rmse: 1.54251\n",
      "[1900]\ttraining's rmse: 1.36636\tvalid_1's rmse: 1.54259\n",
      "[2000]\ttraining's rmse: 1.35888\tvalid_1's rmse: 1.54257\n",
      "Early stopping, best iteration is:\n",
      "[1712]\ttraining's rmse: 1.38074\tvalid_1's rmse: 1.54244\n",
      "[-0.33699112 -0.2870276  -0.48097092 ...  0.87052032 -0.62672583\n",
      "  0.14393601]\n",
      "CV mean score: 1.5489, std: 0.0139.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nlgb, predictions_nlgb, scores_nlgb = train_model(X_ntrain, X_test, y_ntrain, params=lgb_params, folds=folds, model_type='lgb', eval_type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 分类模型 ==========\n",
      "Fold 0 started at Wed Oct 28 09:53:52 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0343282\tvalid_1's binary_logloss: 0.0493825\n",
      "[200]\ttraining's binary_logloss: 0.0269865\tvalid_1's binary_logloss: 0.0473593\n",
      "[300]\ttraining's binary_logloss: 0.0224413\tvalid_1's binary_logloss: 0.0468609\n",
      "[400]\ttraining's binary_logloss: 0.0193353\tvalid_1's binary_logloss: 0.0468397\n",
      "[500]\ttraining's binary_logloss: 0.0168783\tvalid_1's binary_logloss: 0.047034\n",
      "[600]\ttraining's binary_logloss: 0.0148354\tvalid_1's binary_logloss: 0.0473121\n",
      "Early stopping, best iteration is:\n",
      "[326]\ttraining's binary_logloss: 0.0215509\tvalid_1's binary_logloss: 0.0468241\n",
      "[0.00688449 0.00028527 0.00182252 ... 0.00121513 0.01093285 0.00057832]\n",
      "Fold 1 started at Wed Oct 28 09:57:03 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0351193\tvalid_1's binary_logloss: 0.0455575\n",
      "[200]\ttraining's binary_logloss: 0.027642\tvalid_1's binary_logloss: 0.043582\n",
      "[300]\ttraining's binary_logloss: 0.0230509\tvalid_1's binary_logloss: 0.0430319\n",
      "[400]\ttraining's binary_logloss: 0.019891\tvalid_1's binary_logloss: 0.0429293\n",
      "[500]\ttraining's binary_logloss: 0.0174297\tvalid_1's binary_logloss: 0.0430021\n",
      "[600]\ttraining's binary_logloss: 0.0153643\tvalid_1's binary_logloss: 0.0431776\n",
      "Early stopping, best iteration is:\n",
      "[385]\ttraining's binary_logloss: 0.0203152\tvalid_1's binary_logloss: 0.0429256\n",
      "[0.01656694 0.00057193 0.00341526 ... 0.00245982 0.02244854 0.0012078 ]\n",
      "Fold 2 started at Wed Oct 28 10:00:33 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0349061\tvalid_1's binary_logloss: 0.0468874\n",
      "[200]\ttraining's binary_logloss: 0.0275981\tvalid_1's binary_logloss: 0.0449366\n",
      "[300]\ttraining's binary_logloss: 0.0229947\tvalid_1's binary_logloss: 0.0444431\n",
      "[400]\ttraining's binary_logloss: 0.0197943\tvalid_1's binary_logloss: 0.0443122\n",
      "[500]\ttraining's binary_logloss: 0.017288\tvalid_1's binary_logloss: 0.0444194\n",
      "[600]\ttraining's binary_logloss: 0.0152077\tvalid_1's binary_logloss: 0.0445832\n",
      "[700]\ttraining's binary_logloss: 0.0134501\tvalid_1's binary_logloss: 0.0448289\n",
      "Early stopping, best iteration is:\n",
      "[419]\ttraining's binary_logloss: 0.019272\tvalid_1's binary_logloss: 0.044289\n",
      "[0.02379082 0.00078407 0.0052519  ... 0.00380507 0.03566302 0.00176024]\n",
      "Fold 3 started at Wed Oct 28 10:04:17 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.035648\tvalid_1's binary_logloss: 0.0439788\n",
      "[200]\ttraining's binary_logloss: 0.0280222\tvalid_1's binary_logloss: 0.0420793\n",
      "[300]\ttraining's binary_logloss: 0.0233883\tvalid_1's binary_logloss: 0.0416103\n",
      "[400]\ttraining's binary_logloss: 0.0201819\tvalid_1's binary_logloss: 0.0415021\n",
      "[500]\ttraining's binary_logloss: 0.0176541\tvalid_1's binary_logloss: 0.0416121\n",
      "[600]\ttraining's binary_logloss: 0.0155423\tvalid_1's binary_logloss: 0.0417922\n",
      "[700]\ttraining's binary_logloss: 0.0137441\tvalid_1's binary_logloss: 0.0419717\n",
      "Early stopping, best iteration is:\n",
      "[402]\ttraining's binary_logloss: 0.0201286\tvalid_1's binary_logloss: 0.0414996\n",
      "[0.0318831  0.00116066 0.00625915 ... 0.00540471 0.05389594 0.00214651]\n",
      "Fold 4 started at Wed Oct 28 10:07:54 2020\n",
      "Training until validation scores don't improve for 300 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0345196\tvalid_1's binary_logloss: 0.0485146\n",
      "[200]\ttraining's binary_logloss: 0.0271281\tvalid_1's binary_logloss: 0.0464919\n",
      "[300]\ttraining's binary_logloss: 0.0226642\tvalid_1's binary_logloss: 0.0459833\n",
      "[400]\ttraining's binary_logloss: 0.0195237\tvalid_1's binary_logloss: 0.0459607\n",
      "[500]\ttraining's binary_logloss: 0.0170274\tvalid_1's binary_logloss: 0.0461084\n",
      "[600]\ttraining's binary_logloss: 0.0149716\tvalid_1's binary_logloss: 0.0463501\n",
      "Early stopping, best iteration is:\n",
      "[354]\ttraining's binary_logloss: 0.0208544\tvalid_1's binary_logloss: 0.0459298\n",
      "[0.03690222 0.00167164 0.00804803 ... 0.00695516 0.06350124 0.00278562]\n",
      "CV mean score: 0.0443, std: 0.0019.\n"
     ]
    }
   ],
   "source": [
    "print('='*10,'分类模型','='*10)\n",
    "lgb_params['objective'] = 'binary'\n",
    "lgb_params['metric']    = 'binary_logloss'\n",
    "oof_blgb, predictions_blgb, scores_blgb = train_model(X_train , X_test, y_train_binary, params=lgb_params, folds=folds, model_type='lgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_lgb\n",
    "sub_df.to_csv('predictions_lgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_lgb  = pd.DataFrame(oof_lgb)\n",
    "oof_nlgb = pd.DataFrame(oof_nlgb)\n",
    "oof_blgb = pd.DataFrame(oof_blgb)\n",
    "\n",
    "predictions_lgb  = pd.DataFrame(predictions_lgb)\n",
    "predictions_nlgb = pd.DataFrame(predictions_nlgb)\n",
    "predictions_blgb = pd.DataFrame(predictions_blgb)\n",
    "\n",
    "oof_lgb.to_csv('./result/oof_lgb.csv',header=None,index=False)\n",
    "oof_blgb.to_csv('./result/oof_blgb.csv',header=None,index=False)\n",
    "oof_nlgb.to_csv('./result/oof_nlgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_lgb.to_csv('./result/predictions_lgb.csv',header=None,index=False)\n",
    "predictions_nlgb.to_csv('./result/predictions_nlgb.csv',header=None,index=False)\n",
    "predictions_blgb.to_csv('./result/predictions_blgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 10:19:44 2020\n",
      "[0]\ttrain-rmse:3.90147\tvalid_data-rmse:4.07115\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.11469\tvalid_data-rmse:3.79863\n",
      "[200]\ttrain-rmse:2.96452\tvalid_data-rmse:3.80642\n",
      "Stopping. Best iteration:\n",
      "[65]\ttrain-rmse:3.1962\tvalid_data-rmse:3.79636\n",
      "\n",
      "[-0.1133067  -0.02647606 -0.10511222 ...  0.06325655 -0.31075925\n",
      "  0.01708712]\n",
      "Fold 1 started at Wed Oct 28 10:37:16 2020\n",
      "[0]\ttrain-rmse:3.93109\tvalid_data-rmse:3.79406\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.14909\tvalid_data-rmse:3.55252\n",
      "[200]\ttrain-rmse:2.98456\tvalid_data-rmse:3.55923\n",
      "Stopping. Best iteration:\n",
      "[78]\ttrain-rmse:3.1985\tvalid_data-rmse:3.55223\n",
      "\n",
      "[-0.34321851 -0.05176229 -0.21164787 ...  0.14933351 -0.69948253\n",
      "  0.00781279]\n",
      "Fold 2 started at Wed Oct 28 10:55:38 2020\n",
      "[0]\ttrain-rmse:3.93148\tvalid_data-rmse:3.7863\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.13903\tvalid_data-rmse:3.54806\n",
      "[200]\ttrain-rmse:2.9831\tvalid_data-rmse:3.55721\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-rmse:3.2336\tvalid_data-rmse:3.54266\n",
      "\n",
      "[-0.72596431 -0.06134721 -0.31879723 ...  0.20144832 -1.01516548\n",
      "  0.02965641]\n",
      "Fold 3 started at Wed Oct 28 11:13:06 2020\n",
      "[0]\ttrain-rmse:3.9117\tvalid_data-rmse:3.97562\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.13913\tvalid_data-rmse:3.68623\n",
      "[200]\ttrain-rmse:2.97405\tvalid_data-rmse:3.69351\n",
      "Stopping. Best iteration:\n",
      "[83]\ttrain-rmse:3.17301\tvalid_data-rmse:3.6841\n",
      "\n",
      "[-0.97164306 -0.09624885 -0.4494669  ...  0.26486361 -1.31612977\n",
      "  0.02102653]\n",
      "Fold 4 started at Wed Oct 28 11:31:47 2020\n",
      "[0]\ttrain-rmse:3.91012\tvalid_data-rmse:3.99945\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.12513\tvalid_data-rmse:3.68951\n",
      "[200]\ttrain-rmse:2.96439\tvalid_data-rmse:3.69383\n",
      "[300]\ttrain-rmse:2.82236\tvalid_data-rmse:3.69469\n",
      "Stopping. Best iteration:\n",
      "[110]\ttrain-rmse:3.10859\tvalid_data-rmse:3.68826\n",
      "\n",
      "[-1.15335184 -0.12625575 -0.58508685 ...  0.32771052 -1.65109879\n",
      "  0.0266334 ]\n",
      "Fold 5 started at Wed Oct 28 11:52:10 2020\n",
      "[0]\ttrain-rmse:3.9263\tvalid_data-rmse:3.85015\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.14765\tvalid_data-rmse:3.57895\n",
      "[200]\ttrain-rmse:2.97804\tvalid_data-rmse:3.58318\n",
      "Stopping. Best iteration:\n",
      "[68]\ttrain-rmse:3.21975\tvalid_data-rmse:3.57716\n",
      "\n",
      "[-1.26884992 -0.19162094 -0.68672919 ...  0.39081753 -1.93182826\n",
      "  0.05885038]\n",
      "Fold 6 started at Wed Oct 28 12:09:49 2020\n",
      "[0]\ttrain-rmse:3.91812\tvalid_data-rmse:3.92644\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.15131\tvalid_data-rmse:3.65042\n",
      "[200]\ttrain-rmse:2.98662\tvalid_data-rmse:3.65268\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-rmse:3.23721\tvalid_data-rmse:3.64822\n",
      "\n",
      "[-1.42750202 -0.21386804 -0.79120532 ...  0.45730438 -2.12702826\n",
      "  0.02417105]\n",
      "Fold 7 started at Wed Oct 28 12:27:14 2020\n",
      "[0]\ttrain-rmse:3.92026\tvalid_data-rmse:3.90382\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.15817\tvalid_data-rmse:3.59547\n",
      "[200]\ttrain-rmse:2.97363\tvalid_data-rmse:3.6017\n",
      "[300]\ttrain-rmse:2.83474\tvalid_data-rmse:3.60456\n",
      "Stopping. Best iteration:\n",
      "[100]\ttrain-rmse:3.15817\tvalid_data-rmse:3.59547\n",
      "\n",
      "[-1.5699403  -0.22612509 -0.97468928 ...  0.54697597 -2.63974062\n",
      "  0.02545988]\n",
      "Fold 8 started at Wed Oct 28 12:46:53 2020\n",
      "[0]\ttrain-rmse:3.9238\tvalid_data-rmse:3.86613\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.14561\tvalid_data-rmse:3.61483\n",
      "[200]\ttrain-rmse:2.99482\tvalid_data-rmse:3.61974\n",
      "Stopping. Best iteration:\n",
      "[74]\ttrain-rmse:3.20287\tvalid_data-rmse:3.61232\n",
      "\n",
      "[-1.65724716 -0.25635861 -1.05535191 ...  0.6249223  -3.01085135\n",
      "  0.03955088]\n",
      "Fold 9 started at Wed Oct 28 13:04:52 2020\n",
      "[0]\ttrain-rmse:3.8982\tvalid_data-rmse:4.08743\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:3.12313\tvalid_data-rmse:3.80389\n",
      "[200]\ttrain-rmse:2.95722\tvalid_data-rmse:3.80453\n",
      "[300]\ttrain-rmse:2.81346\tvalid_data-rmse:3.80547\n",
      "Stopping. Best iteration:\n",
      "[121]\ttrain-rmse:3.09033\tvalid_data-rmse:3.80335\n",
      "\n",
      "[-1.98011434 -0.29190884 -1.17075564 ...  0.72394169 -3.2349223\n",
      "  0.05324118]\n",
      "CV mean score: 3.6500, std: 0.0885.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 13:25:54 2020\n",
      "[0]\ttrain-rmse:1.77494\tvalid_data-rmse:1.76896\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36476\tvalid_data-rmse:1.54494\n",
      "[200]\ttrain-rmse:1.30514\tvalid_data-rmse:1.54487\n",
      "[300]\ttrain-rmse:1.24384\tvalid_data-rmse:1.54631\n",
      "Stopping. Best iteration:\n",
      "[148]\ttrain-rmse:1.33437\tvalid_data-rmse:1.54433\n",
      "\n",
      "[-0.0256715  -0.01215945 -0.04821761 ...  0.07876629 -0.07210688\n",
      "  0.03778028]\n",
      "Fold 1 started at Wed Oct 28 13:48:53 2020\n",
      "[0]\ttrain-rmse:1.77319\tvalid_data-rmse:1.78689\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36184\tvalid_data-rmse:1.56607\n",
      "[200]\ttrain-rmse:1.30192\tvalid_data-rmse:1.56699\n",
      "[300]\ttrain-rmse:1.24554\tvalid_data-rmse:1.56891\n",
      "Stopping. Best iteration:\n",
      "[117]\ttrain-rmse:1.35048\tvalid_data-rmse:1.56594\n",
      "\n",
      "[-0.05387658 -0.0424792  -0.11209994 ...  0.18521491 -0.1308172\n",
      "  0.0593154 ]\n",
      "Fold 2 started at Wed Oct 28 14:09:54 2020\n",
      "[0]\ttrain-rmse:1.77393\tvalid_data-rmse:1.77817\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36777\tvalid_data-rmse:1.55754\n",
      "[200]\ttrain-rmse:1.30963\tvalid_data-rmse:1.55971\n",
      "Stopping. Best iteration:\n",
      "[87]\ttrain-rmse:1.37749\tvalid_data-rmse:1.557\n",
      "\n",
      "[-0.08617724 -0.06563023 -0.17957301 ...  0.25831765 -0.18178061\n",
      "  0.0821587 ]\n",
      "Fold 3 started at Wed Oct 28 14:29:02 2020\n",
      "[0]\ttrain-rmse:1.77439\tvalid_data-rmse:1.7748\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36516\tvalid_data-rmse:1.54664\n",
      "[200]\ttrain-rmse:1.2992\tvalid_data-rmse:1.54641\n",
      "[300]\ttrain-rmse:1.2439\tvalid_data-rmse:1.5478\n",
      "Stopping. Best iteration:\n",
      "[148]\ttrain-rmse:1.33159\tvalid_data-rmse:1.54574\n",
      "\n",
      "[-0.13836135 -0.080901   -0.2375723  ...  0.33881837 -0.22976879\n",
      "  0.09665322]\n",
      "Fold 4 started at Wed Oct 28 14:52:09 2020\n",
      "[0]\ttrain-rmse:1.77679\tvalid_data-rmse:1.75353\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36725\tvalid_data-rmse:1.5376\n",
      "[200]\ttrain-rmse:1.30779\tvalid_data-rmse:1.53845\n",
      "[300]\ttrain-rmse:1.25034\tvalid_data-rmse:1.54059\n",
      "Stopping. Best iteration:\n",
      "[133]\ttrain-rmse:1.34689\tvalid_data-rmse:1.53738\n",
      "\n",
      "[-0.17256158 -0.10126569 -0.30757502 ...  0.40042875 -0.2713907\n",
      "  0.12284927]\n",
      "Fold 5 started at Wed Oct 28 15:14:10 2020\n",
      "[0]\ttrain-rmse:1.77378\tvalid_data-rmse:1.78123\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36517\tvalid_data-rmse:1.55263\n",
      "[200]\ttrain-rmse:1.30236\tvalid_data-rmse:1.5531\n",
      "Stopping. Best iteration:\n",
      "[95]\ttrain-rmse:1.36866\tvalid_data-rmse:1.55246\n",
      "\n",
      "[-0.19141828 -0.11668382 -0.35726617 ...  0.48444443 -0.32132644\n",
      "  0.14979611]\n",
      "Fold 6 started at Wed Oct 28 15:33:44 2020\n",
      "[0]\ttrain-rmse:1.77245\tvalid_data-rmse:1.78908\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36609\tvalid_data-rmse:1.55589\n",
      "[200]\ttrain-rmse:1.30366\tvalid_data-rmse:1.5562\n",
      "[300]\ttrain-rmse:1.24265\tvalid_data-rmse:1.55749\n",
      "Stopping. Best iteration:\n",
      "[145]\ttrain-rmse:1.33617\tvalid_data-rmse:1.5554\n",
      "\n",
      "[-0.22584418 -0.12945411 -0.39433189 ...  0.53358872 -0.39832277\n",
      "  0.1769059 ]\n",
      "Fold 7 started at Wed Oct 28 15:56:36 2020\n",
      "[0]\ttrain-rmse:1.77196\tvalid_data-rmse:1.7977\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.3624\tvalid_data-rmse:1.56535\n",
      "[200]\ttrain-rmse:1.30159\tvalid_data-rmse:1.5655\n",
      "[300]\ttrain-rmse:1.24393\tvalid_data-rmse:1.56723\n",
      "Stopping. Best iteration:\n",
      "[115]\ttrain-rmse:1.35416\tvalid_data-rmse:1.56455\n",
      "\n",
      "[-0.26059016 -0.1382231  -0.42889716 ...  0.62269276 -0.46986173\n",
      "  0.18898847]\n",
      "Fold 8 started at Wed Oct 28 16:17:32 2020\n",
      "[0]\ttrain-rmse:1.77665\tvalid_data-rmse:1.75335\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36293\tvalid_data-rmse:1.53197\n",
      "[200]\ttrain-rmse:1.30467\tvalid_data-rmse:1.53269\n",
      "[300]\ttrain-rmse:1.24712\tvalid_data-rmse:1.5336\n",
      "Stopping. Best iteration:\n",
      "[155]\ttrain-rmse:1.33075\tvalid_data-rmse:1.53166\n",
      "\n",
      "[-0.28394192 -0.17281964 -0.5229665  ...  0.70534464 -0.53490985\n",
      "  0.20288131]\n",
      "Fold 9 started at Wed Oct 28 16:41:05 2020\n",
      "[0]\ttrain-rmse:1.7728\tvalid_data-rmse:1.78989\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:1.36037\tvalid_data-rmse:1.56762\n",
      "[200]\ttrain-rmse:1.29796\tvalid_data-rmse:1.56824\n",
      "[300]\ttrain-rmse:1.23787\tvalid_data-rmse:1.56929\n",
      "Stopping. Best iteration:\n",
      "[115]\ttrain-rmse:1.34871\tvalid_data-rmse:1.56722\n",
      "\n",
      "[-0.32042447 -0.16589155 -0.57607796 ...  0.78720741 -0.61011326\n",
      "  0.20665139]\n",
      "CV mean score: 1.5522, std: 0.0116.\n",
      "========== 分类模型 ==========\n",
      "Fold 0 started at Wed Oct 28 17:02:04 2020\n",
      "[0]\ttrain-rmse:0.476056\tvalid_data-rmse:0.476129\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095142\tvalid_data-rmse:0.104091\n",
      "[200]\ttrain-rmse:0.090683\tvalid_data-rmse:0.104181\n",
      "[300]\ttrain-rmse:0.086095\tvalid_data-rmse:0.104147\n",
      "Stopping. Best iteration:\n",
      "[149]\ttrain-rmse:0.092949\tvalid_data-rmse:0.10397\n",
      "\n",
      "[0.00404618 0.00036357 0.00124998 ... 0.00079783 0.00574926 0.00058986]\n",
      "Fold 1 started at Wed Oct 28 17:21:47 2020\n",
      "[0]\ttrain-rmse:0.476074\tvalid_data-rmse:0.476038\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095882\tvalid_data-rmse:0.096114\n",
      "[200]\ttrain-rmse:0.091381\tvalid_data-rmse:0.096093\n",
      "[300]\ttrain-rmse:0.086728\tvalid_data-rmse:0.096215\n",
      "Stopping. Best iteration:\n",
      "[125]\ttrain-rmse:0.09482\tvalid_data-rmse:0.095937\n",
      "\n",
      "[0.00868662 0.00071948 0.00227684 ... 0.00160079 0.01409516 0.00114022]\n",
      "Fold 2 started at Wed Oct 28 17:40:12 2020\n",
      "[0]\ttrain-rmse:0.476079\tvalid_data-rmse:0.476018\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095967\tvalid_data-rmse:0.095499\n",
      "[200]\ttrain-rmse:0.091342\tvalid_data-rmse:0.09587\n",
      "[300]\ttrain-rmse:0.086671\tvalid_data-rmse:0.096204\n",
      "Stopping. Best iteration:\n",
      "[114]\ttrain-rmse:0.095301\tvalid_data-rmse:0.095491\n",
      "\n",
      "[0.01357797 0.0010772  0.00343823 ... 0.00250653 0.02152826 0.00182314]\n",
      "Fold 3 started at Wed Oct 28 17:58:02 2020\n",
      "[0]\ttrain-rmse:0.476063\tvalid_data-rmse:0.476094\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095612\tvalid_data-rmse:0.100234\n",
      "[200]\ttrain-rmse:0.091178\tvalid_data-rmse:0.100496\n",
      "[300]\ttrain-rmse:0.086531\tvalid_data-rmse:0.100558\n",
      "Stopping. Best iteration:\n",
      "[110]\ttrain-rmse:0.095217\tvalid_data-rmse:0.100159\n",
      "\n",
      "[0.01804448 0.0014738  0.00472777 ... 0.00370722 0.03075859 0.00264444]\n",
      "Fold 4 started at Wed Oct 28 18:15:49 2020\n",
      "[0]\ttrain-rmse:0.476066\tvalid_data-rmse:0.476084\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095494\tvalid_data-rmse:0.101127\n",
      "[200]\ttrain-rmse:0.091252\tvalid_data-rmse:0.101042\n",
      "[300]\ttrain-rmse:0.086721\tvalid_data-rmse:0.10109\n",
      "Stopping. Best iteration:\n",
      "[162]\ttrain-rmse:0.092917\tvalid_data-rmse:0.100798\n",
      "\n",
      "[0.02248657 0.00176591 0.00644708 ... 0.00444987 0.03728939 0.00322682]\n",
      "Fold 5 started at Wed Oct 28 18:36:26 2020\n",
      "[0]\ttrain-rmse:0.476069\tvalid_data-rmse:0.476066\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095972\tvalid_data-rmse:0.096914\n",
      "[200]\ttrain-rmse:0.091525\tvalid_data-rmse:0.096621\n",
      "[300]\ttrain-rmse:0.086961\tvalid_data-rmse:0.096411\n",
      "[400]\ttrain-rmse:0.082423\tvalid_data-rmse:0.096596\n",
      "Stopping. Best iteration:\n",
      "[294]\ttrain-rmse:0.087243\tvalid_data-rmse:0.096373\n",
      "\n",
      "[0.02586014 0.00198885 0.00766118 ... 0.00548898 0.04651267 0.00357894]\n",
      "Fold 6 started at Wed Oct 28 19:04:50 2020\n",
      "[0]\ttrain-rmse:0.476071\tvalid_data-rmse:0.476071\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095742\tvalid_data-rmse:0.098903\n",
      "[200]\ttrain-rmse:0.091609\tvalid_data-rmse:0.099044\n",
      "[300]\ttrain-rmse:0.086962\tvalid_data-rmse:0.099359\n",
      "Stopping. Best iteration:\n",
      "[126]\ttrain-rmse:0.094721\tvalid_data-rmse:0.098884\n",
      "\n",
      "[0.03105726 0.00226785 0.00902762 ... 0.00658674 0.05231658 0.00412364]\n",
      "Fold 7 started at Wed Oct 28 19:23:21 2020\n",
      "[0]\ttrain-rmse:0.476063\tvalid_data-rmse:0.47603\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095896\tvalid_data-rmse:0.098037\n",
      "[200]\ttrain-rmse:0.091678\tvalid_data-rmse:0.098021\n",
      "[300]\ttrain-rmse:0.087124\tvalid_data-rmse:0.098242\n",
      "Stopping. Best iteration:\n",
      "[157]\ttrain-rmse:0.093593\tvalid_data-rmse:0.097865\n",
      "\n",
      "[0.03549035 0.00253236 0.01057243 ... 0.00755154 0.0590746  0.00465402]\n",
      "Fold 8 started at Wed Oct 28 19:43:41 2020\n",
      "[0]\ttrain-rmse:0.476067\tvalid_data-rmse:0.476053\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095906\tvalid_data-rmse:0.097955\n",
      "[200]\ttrain-rmse:0.091534\tvalid_data-rmse:0.097808\n",
      "[300]\ttrain-rmse:0.087013\tvalid_data-rmse:0.097994\n",
      "Stopping. Best iteration:\n",
      "[168]\ttrain-rmse:0.092969\tvalid_data-rmse:0.097743\n",
      "\n",
      "[0.03816239 0.00277869 0.01178081 ... 0.00840982 0.06535    0.00515612]\n",
      "Fold 9 started at Wed Oct 28 20:04:36 2020\n",
      "[0]\ttrain-rmse:0.476051\tvalid_data-rmse:0.476137\n",
      "Multiple eval metrics have been passed: 'valid_data-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid_data-rmse hasn't improved in 200 rounds.\n",
      "[100]\ttrain-rmse:0.095143\tvalid_data-rmse:0.104176\n",
      "[200]\ttrain-rmse:0.090864\tvalid_data-rmse:0.104112\n",
      "[300]\ttrain-rmse:0.08625\tvalid_data-rmse:0.104039\n",
      "[400]\ttrain-rmse:0.08165\tvalid_data-rmse:0.1042\n",
      "Stopping. Best iteration:\n",
      "[297]\ttrain-rmse:0.086376\tvalid_data-rmse:0.104017\n",
      "\n",
      "[0.04078136 0.0030213  0.01303061 ... 0.00895933 0.07314474 0.00557031]\n",
      "CV mean score: 0.0441, std: 0.0021.\n"
     ]
    }
   ],
   "source": [
    "#### xgb\n",
    "xgb_params = {'eta':0.05, 'max_leaves':47, 'max_depth':10, 'subsample':0.8, 'colsample_bytree':0.8,\n",
    "              'min_child_weight':40, 'max_bin':128, 'reg_alpha':2.0, 'reg_lambda':2.0, \n",
    "              'objective':'reg:linear', 'eval_metric':'rmse', 'silent': True, 'nthread':4}\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_xgb , predictions_xgb , scores_xgb  = train_model(X_train , X_test, y_train , params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_nxgb, predictions_nxgb, scores_nxgb = train_model(X_ntrain, X_test, y_ntrain, params=xgb_params, folds=folds, model_type='xgb', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "xgb_params['objective'] = 'binary:logistic'\n",
    "xgb_params['metric']    = 'binary_logloss'\n",
    "oof_bxgb, predictions_bxgb, scores_bxgb = train_model(X_train , X_test, y_train_binary, params=xgb_params, folds=folds, model_type='xgb', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_xgb\n",
    "sub_df.to_csv('predictions_xgb.csv', index=False)\n",
    "\n",
    "oof_xgb  = pd.DataFrame(oof_xgb)\n",
    "oof_nxgb = pd.DataFrame(oof_nxgb)\n",
    "oof_bxgb = pd.DataFrame(oof_bxgb)\n",
    "\n",
    "predictions_xgb  = pd.DataFrame(predictions_xgb)\n",
    "predictions_nxgb = pd.DataFrame(predictions_nxgb)\n",
    "predictions_bxgb = pd.DataFrame(predictions_bxgb)\n",
    "\n",
    "oof_xgb.to_csv('./result/oof_xgb.csv',header=None,index=False)\n",
    "oof_bxgb.to_csv('./result/oof_bxgb.csv',header=None,index=False)\n",
    "oof_nxgb.to_csv('./result/oof_nxgb.csv',header=None,index=False)\n",
    "\n",
    "predictions_xgb.to_csv('./result/predictions_xgb.csv',header=None,index=False)\n",
    "predictions_nxgb.to_csv('./result/predictions_nxgb.csv',header=None,index=False)\n",
    "predictions_bxgb.to_csv('./result/predictions_bxgb.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 20:33:09 2020\n",
      "0:\tlearn: 3.8519465\ttest: 3.8953529\tbest: 3.8953529 (0)\ttotal: 346ms\tremaining: 1h 55m 22s\n",
      "100:\tlearn: 3.5787031\ttest: 3.6830510\tbest: 3.6830510 (100)\ttotal: 33.5s\tremaining: 1h 49m 50s\n",
      "200:\tlearn: 3.5356349\ttest: 3.6776853\tbest: 3.6773519 (182)\ttotal: 1m 4s\tremaining: 1h 46m 35s\n",
      "300:\tlearn: 3.5009895\ttest: 3.6738267\tbest: 3.6733028 (271)\ttotal: 1m 35s\tremaining: 1h 44m 17s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.673302781\n",
      "bestIteration = 271\n",
      "\n",
      "Shrink model to first 272 iterations.\n",
      "[-0.34994941 -0.05174866 -0.09176241 ...  0.0861785  -0.3049672\n",
      "  0.01850385]\n",
      "Fold 1 started at Wed Oct 28 20:35:37 2020\n",
      "0:\tlearn: 3.8614805\ttest: 3.8031024\tbest: 3.8031024 (0)\ttotal: 329ms\tremaining: 1h 49m 42s\n",
      "100:\tlearn: 3.5930913\ttest: 3.5944800\tbest: 3.5944155 (97)\ttotal: 35.3s\tremaining: 1h 55m 55s\n",
      "200:\tlearn: 3.5465234\ttest: 3.5868264\tbest: 3.5868264 (200)\ttotal: 1m 9s\tremaining: 1h 53m 46s\n",
      "300:\tlearn: 3.5167499\ttest: 3.5846692\tbest: 3.5846049 (291)\ttotal: 1m 42s\tremaining: 1h 52m 13s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.584528718\n",
      "bestIteration = 305\n",
      "\n",
      "Shrink model to first 306 iterations.\n",
      "[-0.6751308  -0.08160316 -0.18480328 ...  0.17479172 -0.60728466\n",
      "  0.04128466]\n",
      "Fold 2 started at Wed Oct 28 20:38:26 2020\n",
      "0:\tlearn: 3.8355016\ttest: 4.0361364\tbest: 4.0361364 (0)\ttotal: 280ms\tremaining: 1h 33m 27s\n",
      "100:\tlearn: 3.5670686\ttest: 3.8152115\tbest: 3.8152115 (100)\ttotal: 33.6s\tremaining: 1h 50m 16s\n",
      "200:\tlearn: 3.5256957\ttest: 3.8055369\tbest: 3.8054973 (197)\ttotal: 1m 5s\tremaining: 1h 48m 15s\n",
      "300:\tlearn: 3.4950221\ttest: 3.8019574\tbest: 3.8019574 (300)\ttotal: 1m 37s\tremaining: 1h 46m 42s\n",
      "400:\tlearn: 3.4655938\ttest: 3.7996838\tbest: 3.7996364 (399)\ttotal: 2m 9s\tremaining: 1h 45m 30s\n",
      "500:\tlearn: 3.4307967\ttest: 3.7977939\tbest: 3.7977939 (500)\ttotal: 2m 41s\tremaining: 1h 44m 50s\n",
      "600:\tlearn: 3.3999157\ttest: 3.7961849\tbest: 3.7961849 (600)\ttotal: 3m 13s\tremaining: 1h 44m 12s\n",
      "700:\tlearn: 3.3641889\ttest: 3.7951851\tbest: 3.7951273 (697)\ttotal: 3m 45s\tremaining: 1h 43m 17s\n",
      "800:\tlearn: 3.3349215\ttest: 3.7954522\tbest: 3.7946296 (780)\ttotal: 4m 16s\tremaining: 1h 42m 23s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.794629552\n",
      "bestIteration = 780\n",
      "\n",
      "Shrink model to first 781 iterations.\n",
      "[-1.01763841 -0.12246797 -0.27304173 ...  0.24248031 -0.92088788\n",
      "  0.06630601]\n",
      "Fold 3 started at Wed Oct 28 20:43:40 2020\n",
      "0:\tlearn: 3.8739505\ttest: 3.6767430\tbest: 3.6767430 (0)\ttotal: 276ms\tremaining: 1h 31m 53s\n",
      "100:\tlearn: 3.5938950\ttest: 3.5164039\tbest: 3.5164039 (100)\ttotal: 33.6s\tremaining: 1h 50m 29s\n",
      "200:\tlearn: 3.5516786\ttest: 3.5119152\tbest: 3.5119148 (199)\ttotal: 1m 6s\tremaining: 1h 49m 33s\n",
      "300:\tlearn: 3.5224696\ttest: 3.5100807\tbest: 3.5095796 (261)\ttotal: 1m 39s\tremaining: 1h 48m 39s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.509579643\n",
      "bestIteration = 261\n",
      "\n",
      "Shrink model to first 262 iterations.\n",
      "[-1.32923584 -0.15709066 -0.35677705 ...  0.32500923 -1.26353485\n",
      "  0.06640201]\n",
      "Fold 4 started at Wed Oct 28 20:46:09 2020\n",
      "0:\tlearn: 3.8632399\ttest: 3.7920509\tbest: 3.7920509 (0)\ttotal: 328ms\tremaining: 1h 49m 22s\n",
      "100:\tlearn: 3.5834667\ttest: 3.6194639\tbest: 3.6194639 (100)\ttotal: 36.3s\tremaining: 1h 59m 9s\n",
      "200:\tlearn: 3.5377982\ttest: 3.6167607\tbest: 3.6166587 (155)\ttotal: 1m 10s\tremaining: 1h 56m 10s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.616658726\n",
      "bestIteration = 155\n",
      "\n",
      "Shrink model to first 156 iterations.\n",
      "[-1.64291684 -0.20100727 -0.41945822 ...  0.40269968 -1.59280728\n",
      "  0.08022819]\n",
      "Fold 5 started at Wed Oct 28 20:48:10 2020\n",
      "0:\tlearn: 3.8477596\ttest: 3.9210841\tbest: 3.9210841 (0)\ttotal: 296ms\tremaining: 1h 38m 39s\n",
      "100:\tlearn: 3.5765028\ttest: 3.7134673\tbest: 3.7134673 (100)\ttotal: 35.4s\tremaining: 1h 56m 13s\n",
      "200:\tlearn: 3.5300774\ttest: 3.7071455\tbest: 3.7071082 (197)\ttotal: 1m 8s\tremaining: 1h 52m 13s\n",
      "300:\tlearn: 3.4973006\ttest: 3.7052054\tbest: 3.7050411 (265)\ttotal: 1m 41s\tremaining: 1h 50m 44s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.705041063\n",
      "bestIteration = 265\n",
      "\n",
      "Shrink model to first 266 iterations.\n",
      "[-1.98766668 -0.24391232 -0.52123948 ...  0.4701224  -1.85323317\n",
      "  0.10431593]\n",
      "Fold 6 started at Wed Oct 28 20:50:43 2020\n",
      "0:\tlearn: 3.8524461\ttest: 3.8801269\tbest: 3.8801269 (0)\ttotal: 303ms\tremaining: 1h 40m 57s\n",
      "100:\tlearn: 3.5791195\ttest: 3.6967657\tbest: 3.6967657 (100)\ttotal: 32.6s\tremaining: 1h 47m 1s\n",
      "200:\tlearn: 3.5352709\ttest: 3.6945322\tbest: 3.6934863 (161)\ttotal: 1m 3s\tremaining: 1h 45m 2s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.693486265\n",
      "bestIteration = 161\n",
      "\n",
      "Shrink model to first 162 iterations.\n",
      "[-2.3304568  -0.27740847 -0.61842997 ...  0.53975278 -2.22152618\n",
      "  0.11408959]\n",
      "Fold 7 started at Wed Oct 28 20:52:38 2020\n",
      "0:\tlearn: 3.8621685\ttest: 3.8001688\tbest: 3.8001688 (0)\ttotal: 282ms\tremaining: 1h 33m 50s\n",
      "100:\tlearn: 3.5830135\ttest: 3.5742012\tbest: 3.5742012 (100)\ttotal: 33.5s\tremaining: 1h 49m 52s\n",
      "200:\tlearn: 3.5414066\ttest: 3.5657436\tbest: 3.5657436 (200)\ttotal: 1m 6s\tremaining: 1h 49m 13s\n",
      "300:\tlearn: 3.5096606\ttest: 3.5609677\tbest: 3.5609677 (300)\ttotal: 1m 39s\tremaining: 1h 48m 22s\n",
      "400:\tlearn: 3.4750429\ttest: 3.5601272\tbest: 3.5598039 (360)\ttotal: 2m 12s\tremaining: 1h 47m 37s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.559803866\n",
      "bestIteration = 360\n",
      "\n",
      "Shrink model to first 361 iterations.\n",
      "[-2.64873804 -0.33420585 -0.70972246 ...  0.61718654 -2.51037788\n",
      "  0.14006584]\n",
      "Fold 8 started at Wed Oct 28 20:55:42 2020\n",
      "0:\tlearn: 3.8643889\ttest: 3.7673386\tbest: 3.7673386 (0)\ttotal: 300ms\tremaining: 1h 40m 9s\n",
      "100:\tlearn: 3.5948246\ttest: 3.5815396\tbest: 3.5814529 (99)\ttotal: 35s\tremaining: 1h 54m 51s\n",
      "200:\tlearn: 3.5463923\ttest: 3.5737640\tbest: 3.5737640 (200)\ttotal: 1m 7s\tremaining: 1h 50m 46s\n",
      "300:\tlearn: 3.5204579\ttest: 3.5709415\tbest: 3.5709238 (299)\ttotal: 1m 40s\tremaining: 1h 49m 33s\n",
      "400:\tlearn: 3.4907911\ttest: 3.5666729\tbest: 3.5666676 (399)\ttotal: 2m 13s\tremaining: 1h 48m 51s\n",
      "500:\tlearn: 3.4554816\ttest: 3.5647414\tbest: 3.5647414 (500)\ttotal: 2m 46s\tremaining: 1h 48m 12s\n",
      "600:\tlearn: 3.4197829\ttest: 3.5636358\tbest: 3.5634815 (594)\ttotal: 3m 20s\tremaining: 1h 47m 42s\n",
      "700:\tlearn: 3.3864771\ttest: 3.5628336\tbest: 3.5624205 (673)\ttotal: 3m 53s\tremaining: 1h 47m 16s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.561921625\n",
      "bestIteration = 726\n",
      "\n",
      "Shrink model to first 727 iterations.\n",
      "[-2.95304562 -0.38346528 -0.81919585 ...  0.70976223 -3.0125269\n",
      "  0.16910807]\n",
      "Fold 9 started at Wed Oct 28 21:00:47 2020\n",
      "0:\tlearn: 3.8408774\ttest: 3.9793495\tbest: 3.9793495 (0)\ttotal: 278ms\tremaining: 1h 32m 30s\n",
      "100:\tlearn: 3.5730146\ttest: 3.7936526\tbest: 3.7936526 (100)\ttotal: 33.3s\tremaining: 1h 49m 15s\n",
      "200:\tlearn: 3.5261108\ttest: 3.7877847\tbest: 3.7877847 (200)\ttotal: 1m 5s\tremaining: 1h 47m 50s\n",
      "300:\tlearn: 3.4934230\ttest: 3.7844573\tbest: 3.7843830 (292)\ttotal: 1m 38s\tremaining: 1h 47m 22s\n",
      "400:\tlearn: 3.4648557\ttest: 3.7826891\tbest: 3.7826891 (400)\ttotal: 2m 10s\tremaining: 1h 46m 21s\n",
      "500:\tlearn: 3.4336557\ttest: 3.7822152\tbest: 3.7821715 (486)\ttotal: 2m 43s\tremaining: 1h 45m 44s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 3.78210366\n",
      "bestIteration = 513\n",
      "\n",
      "Shrink model to first 514 iterations.\n",
      "[-3.26510451 -0.42534925 -0.88638833 ...  0.78581692 -3.44422085\n",
      "  0.18895032]\n",
      "CV mean score: 3.6481, std: 0.0921.\n",
      "========== without outliers 回归模型 ==========\n",
      "Fold 0 started at Wed Oct 28 21:04:39 2020\n",
      "0:\tlearn: 1.7079186\ttest: 1.6998025\tbest: 1.6998025 (0)\ttotal: 317ms\tremaining: 1h 45m 40s\n",
      "100:\tlearn: 1.5444660\ttest: 1.5563617\tbest: 1.5563617 (100)\ttotal: 35.1s\tremaining: 1h 55m 15s\n",
      "200:\tlearn: 1.5252389\ttest: 1.5509716\tbest: 1.5509716 (200)\ttotal: 1m 9s\tremaining: 1h 53m 37s\n",
      "300:\tlearn: 1.5084485\ttest: 1.5484287\tbest: 1.5484077 (299)\ttotal: 1m 42s\tremaining: 1h 52m\n",
      "400:\tlearn: 1.4924599\ttest: 1.5471907\tbest: 1.5471907 (400)\ttotal: 2m 16s\tremaining: 1h 51m\n",
      "500:\tlearn: 1.4783264\ttest: 1.5466907\tbest: 1.5466645 (494)\ttotal: 2m 49s\tremaining: 1h 50m 14s\n",
      "600:\tlearn: 1.4649502\ttest: 1.5463933\tbest: 1.5463249 (593)\ttotal: 3m 23s\tremaining: 1h 49m 26s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.546324948\n",
      "bestIteration = 593\n",
      "\n",
      "Shrink model to first 594 iterations.\n",
      "[-0.03330906 -0.02581416 -0.04329774 ...  0.08903704 -0.08690585\n",
      "  0.0086066 ]\n",
      "Fold 1 started at Wed Oct 28 21:09:04 2020\n",
      "0:\tlearn: 1.7045062\ttest: 1.7296741\tbest: 1.7296741 (0)\ttotal: 315ms\tremaining: 1h 44m 49s\n",
      "100:\tlearn: 1.5420346\ttest: 1.5776467\tbest: 1.5776467 (100)\ttotal: 34.7s\tremaining: 1h 53m 57s\n",
      "200:\tlearn: 1.5230265\ttest: 1.5719512\tbest: 1.5719512 (200)\ttotal: 1m 9s\tremaining: 1h 53m 56s\n",
      "300:\tlearn: 1.5066855\ttest: 1.5692810\tbest: 1.5692610 (298)\ttotal: 1m 44s\tremaining: 1h 53m 45s\n",
      "400:\tlearn: 1.4932846\ttest: 1.5684511\tbest: 1.5683190 (394)\ttotal: 2m 18s\tremaining: 1h 53m 1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.568319014\n",
      "bestIteration = 394\n",
      "\n",
      "Shrink model to first 395 iterations.\n",
      "[-0.0779681  -0.04829348 -0.10250083 ...  0.18131976 -0.15353891\n",
      "  0.03545093]\n",
      "Fold 2 started at Wed Oct 28 21:12:25 2020\n",
      "0:\tlearn: 1.7091747\ttest: 1.6929942\tbest: 1.6929942 (0)\ttotal: 295ms\tremaining: 1h 38m 19s\n",
      "100:\tlearn: 1.5449630\ttest: 1.5525168\tbest: 1.5525168 (100)\ttotal: 35.9s\tremaining: 1h 57m 48s\n",
      "200:\tlearn: 1.5248639\ttest: 1.5476102\tbest: 1.5475889 (199)\ttotal: 1m 10s\tremaining: 1h 56m 22s\n",
      "300:\tlearn: 1.5082965\ttest: 1.5465394\tbest: 1.5464751 (287)\ttotal: 1m 45s\tremaining: 1h 55m 33s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.546115837\n",
      "bestIteration = 337\n",
      "\n",
      "Shrink model to first 338 iterations.\n",
      "[-0.11456809 -0.07879559 -0.13161571 ...  0.27064224 -0.22022675\n",
      "  0.06088595]\n",
      "Fold 3 started at Wed Oct 28 21:15:31 2020\n",
      "0:\tlearn: 1.7081886\ttest: 1.6934901\tbest: 1.6934901 (0)\ttotal: 269ms\tremaining: 1h 29m 46s\n",
      "100:\tlearn: 1.5463234\ttest: 1.5465909\tbest: 1.5465909 (100)\ttotal: 34.4s\tremaining: 1h 53m 5s\n",
      "200:\tlearn: 1.5267156\ttest: 1.5411893\tbest: 1.5411893 (200)\ttotal: 1m 8s\tremaining: 1h 52m 39s\n",
      "300:\tlearn: 1.5106118\ttest: 1.5387807\tbest: 1.5387807 (300)\ttotal: 1m 42s\tremaining: 1h 51m 30s\n",
      "400:\tlearn: 1.4962591\ttest: 1.5376417\tbest: 1.5375769 (398)\ttotal: 2m 15s\tremaining: 1h 50m 19s\n",
      "500:\tlearn: 1.4823238\ttest: 1.5368212\tbest: 1.5368212 (500)\ttotal: 2m 48s\tremaining: 1h 49m 33s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.536807913\n",
      "bestIteration = 502\n",
      "\n",
      "Shrink model to first 503 iterations.\n",
      "[-0.15293345 -0.10677653 -0.17219187 ...  0.37261915 -0.30020008\n",
      "  0.07806014]\n",
      "Fold 4 started at Wed Oct 28 21:19:24 2020\n",
      "0:\tlearn: 1.7101413\ttest: 1.6787506\tbest: 1.6787506 (0)\ttotal: 317ms\tremaining: 1h 45m 36s\n",
      "100:\tlearn: 1.5464675\ttest: 1.5383837\tbest: 1.5383837 (100)\ttotal: 34.6s\tremaining: 1h 53m 38s\n",
      "200:\tlearn: 1.5264037\ttest: 1.5331388\tbest: 1.5331388 (200)\ttotal: 1m 8s\tremaining: 1h 53m 7s\n",
      "300:\tlearn: 1.5105765\ttest: 1.5319594\tbest: 1.5319341 (276)\ttotal: 1m 43s\tremaining: 1h 52m 33s\n",
      "400:\tlearn: 1.4955034\ttest: 1.5314751\tbest: 1.5314554 (399)\ttotal: 2m 17s\tremaining: 1h 52m 4s\n",
      "500:\tlearn: 1.4812347\ttest: 1.5310300\tbest: 1.5310109 (497)\ttotal: 2m 51s\tremaining: 1h 51m 24s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.53096081\n",
      "bestIteration = 511\n",
      "\n",
      "Shrink model to first 512 iterations.\n",
      "[-0.18511148 -0.14474702 -0.23053012 ...  0.47053085 -0.37848037\n",
      "  0.09966762]\n",
      "Fold 5 started at Wed Oct 28 21:23:23 2020\n",
      "0:\tlearn: 1.7085830\ttest: 1.6956167\tbest: 1.6956167 (0)\ttotal: 308ms\tremaining: 1h 42m 47s\n",
      "100:\tlearn: 1.5450989\ttest: 1.5477277\tbest: 1.5477277 (100)\ttotal: 34.7s\tremaining: 1h 53m 54s\n",
      "200:\tlearn: 1.5256923\ttest: 1.5426232\tbest: 1.5425895 (198)\ttotal: 1m 7s\tremaining: 1h 50m 46s\n",
      "300:\tlearn: 1.5099801\ttest: 1.5407554\tbest: 1.5407554 (300)\ttotal: 1m 40s\tremaining: 1h 50m 1s\n",
      "400:\tlearn: 1.4957291\ttest: 1.5399279\tbest: 1.5399039 (396)\ttotal: 2m 14s\tremaining: 1h 49m 17s\n",
      "500:\tlearn: 1.4820936\ttest: 1.5388883\tbest: 1.5388534 (499)\ttotal: 2m 47s\tremaining: 1h 48m 44s\n",
      "600:\tlearn: 1.4692031\ttest: 1.5383444\tbest: 1.5382412 (591)\ttotal: 3m 20s\tremaining: 1h 48m 1s\n",
      "700:\tlearn: 1.4576583\ttest: 1.5381825\tbest: 1.5380656 (693)\ttotal: 3m 53s\tremaining: 1h 47m 22s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 1.538065579\n",
      "bestIteration = 693\n",
      "\n",
      "Shrink model to first 694 iterations.\n",
      "[-0.23166817 -0.17371862 -0.2554213  ...  0.58048011 -0.45042024\n",
      "  0.12841036]\n",
      "Fold 6 started at Wed Oct 28 21:28:17 2020\n",
      "0:\tlearn: 1.7041508\ttest: 1.7366440\tbest: 1.7366440 (0)\ttotal: 315ms\tremaining: 1h 44m 57s\n",
      "100:\tlearn: 1.5430041\ttest: 1.5716286\tbest: 1.5716286 (100)\ttotal: 33.7s\tremaining: 1h 50m 42s\n",
      "200:\tlearn: 1.5236732\ttest: 1.5647617\tbest: 1.5647617 (200)\ttotal: 1m 8s\tremaining: 1h 52m 8s\n",
      "300:\tlearn: 1.5072191\ttest: 1.5618860\tbest: 1.5618860 (300)\ttotal: 1m 42s\tremaining: 1h 51m 49s\n",
      "400:\tlearn: 1.4926878\ttest: 1.5605860\tbest: 1.5605645 (399)\ttotal: 2m 16s\tremaining: 1h 51m 4s\n",
      "500:\tlearn: 1.4789471\ttest: 1.5597245\tbest: 1.5597245 (500)\ttotal: 2m 50s\tremaining: 1h 50m 19s\n"
     ]
    }
   ],
   "source": [
    "#### cat\n",
    "cat_params = {'learning_rate': 0.05, 'depth': 9, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "              'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "folds = KFold(n_splits=10, shuffle=True, random_state=18)\n",
    "print('='*10,'回归模型','='*10)\n",
    "oof_cat , predictions_cat , scores_cat  = train_model(X_train , X_test, y_train , params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'without outliers 回归模型','='*10)\n",
    "oof_ncat, predictions_ncat, scores_ncat = train_model(X_ntrain, X_test, y_ntrain, params=cat_params, folds=folds, model_type='cat', eval_type='regression')\n",
    "print('='*10,'分类模型','='*10)\n",
    "oof_bcat, predictions_bcat, scores_bcat = train_model(X_train , X_test, y_train_binary, params=cat_params, folds=folds, model_type='cat', eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_cat\n",
    "sub_df.to_csv('predictions_cat.csv', index=False)\n",
    "\n",
    "oof_cat  = pd.DataFrame(oof_cat)\n",
    "oof_ncat = pd.DataFrame(oof_ncat)\n",
    "oof_bcat = pd.DataFrame(oof_bcat)\n",
    "\n",
    "predictions_cat  = pd.DataFrame(predictions_cat)\n",
    "predictions_ncat = pd.DataFrame(predictions_ncat)\n",
    "predictions_bcat = pd.DataFrame(predictions_bcat)\n",
    "\n",
    "oof_cat.to_csv('./result/oof_cat.csv',header=None,index=False)\n",
    "oof_bcat.to_csv('./result/oof_bcat.csv',header=None,index=False)\n",
    "oof_ncat.to_csv('./result/oof_ncat.csv',header=None,index=False)\n",
    "\n",
    "predictions_cat.to_csv('./result/predictions_cat.csv',header=None,index=False)\n",
    "predictions_ncat.to_csv('./result/predictions_ncat.csv',header=None,index=False)\n",
    "predictions_bcat.to_csv('./result/predictions_bcat.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### lgb\n",
    "# oof_lgb  = pd.read_csv('./result/oof_lgb.csv',header=None)\n",
    "# oof_nlgb = pd.read_csv('./result/oof_nlgb.csv',header=None)\n",
    "# oof_blgb = pd.read_csv('./result/oof_blgb.csv',header=None)\n",
    "\n",
    "# predictions_lgb  = pd.read_csv('./result/predictions_lgb.csv',header=None)\n",
    "# predictions_nlgb = pd.read_csv('./result/predictions_nlgb.csv',header=None)\n",
    "# predictions_blgb = pd.read_csv('./result/predictions_blgb.csv',header=None)\n",
    "\n",
    "# #### xgb\n",
    "# oof_xgb  = pd.read_csv('./result/oof_xgb.csv',header=None)\n",
    "# oof_nxgb = pd.read_csv('./result/oof_nxgb.csv',header=None)\n",
    "# oof_bxgb = pd.read_csv('./result/oof_bxgb.csv',header=None)\n",
    "\n",
    "# predictions_xgb  = pd.read_csv('./result/predictions_xgb.csv',header=None)\n",
    "# predictions_nxgb = pd.read_csv('./result/predictions_nxgb.csv',header=None)\n",
    "# predictions_bxgb = pd.read_csv('./result/predictions_bxgb.csv',header=None)\n",
    "\n",
    "# #### cat\n",
    "# oof_cat  = pd.read_csv('./result/oof_cat.csv',header=None)\n",
    "# oof_ncat = pd.read_csv('./result/oof_ncat.csv',header=None)\n",
    "# oof_bcat = pd.read_csv('./result/oof_bcat.csv',header=None)\n",
    "\n",
    "# predictions_cat  = pd.read_csv('./result/predictions_cat.csv',header=None)\n",
    "# predictions_ncat = pd.read_csv('./result/predictions_ncat.csv',header=None)\n",
    "# predictions_bcat = pd.read_csv('./result/predictions_bcat.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加权融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_lgb + predictions_xgb + predictions_cat) / 3\n",
    "sub_df.to_csv('predictions_wei_average.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  3.639521012513924\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  1.5478982032202415\n",
      "==============================\n",
      "fold n°1\n",
      "----------Stacking 0----------\n",
      "fold n°2\n",
      "----------Stacking 1----------\n",
      "fold n°3\n",
      "----------Stacking 2----------\n",
      "fold n°4\n",
      "----------Stacking 3----------\n",
      "fold n°5\n",
      "----------Stacking 4----------\n",
      "fold n°6\n",
      "----------Stacking 5----------\n",
      "fold n°7\n",
      "----------Stacking 6----------\n",
      "fold n°8\n",
      "----------Stacking 7----------\n",
      "fold n°9\n",
      "----------Stacking 8----------\n",
      "fold n°10\n",
      "----------Stacking 9----------\n",
      "mean:  0.04393440399019065\n"
     ]
    }
   ],
   "source": [
    "#### stack 回归模型  without-outliers回归模型 分类模型\n",
    "def stack_model(oof_1, oof_2, oof_3, predictions_1, predictions_2, predictions_3, y, eval_type='regression'):\n",
    "   \n",
    "    train_stack = np.vstack([oof_1, oof_2]).transpose()\n",
    "    test_stack = np.vstack([predictions_1, predictions_2]).transpose()\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    folds = RepeatedKFold(n_splits=5, n_repeats=2, random_state=2020)\n",
    "    oof = np.zeros(train_stack.shape[0])\n",
    "    predictions = np.zeros(test_stack.shape[0])\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, y)):\n",
    "        print(\"fold n°{}\".format(fold_+1))\n",
    "        trn_data, trn_y = train_stack[trn_idx], y[trn_idx]\n",
    "        val_data, val_y = train_stack[val_idx], y[val_idx]\n",
    "        print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n",
    "        clf = BayesianRidge()\n",
    "        clf.fit(trn_data, trn_y)\n",
    "\n",
    "        oof[val_idx] = clf.predict(val_data)\n",
    "        predictions += clf.predict(test_stack) / (5 * 2)\n",
    "    if eval_type == 'regression':\n",
    "        print('mean: ',np.sqrt(mean_squared_error(y, oof)))\n",
    "    if eval_type == 'binary':\n",
    "        print('mean: ',log_loss(y, oof))\n",
    "    \n",
    "    return oof, predictions\n",
    "print('='*30)\n",
    "oof_stack , predictions_stack  = stack_model(oof_lgb[0] , oof_xgb[0] , oof_cat[0] , predictions_lgb[0] , predictions_xgb[0] , predictions_cat[0] , target)\n",
    "print('='*30)\n",
    "oof_nstack, predictions_nstack = stack_model(oof_nlgb[0], oof_nxgb[0], oof_ncat[0], predictions_nlgb[0], predictions_nxgb[0], predictions_ncat[0], ntarget)\n",
    "print('='*30)\n",
    "oof_bstack, predictions_bstack = stack_model(oof_blgb[0], oof_bxgb[0], oof_bcat[0], predictions_blgb[0], predictions_bxgb[0], predictions_bcat[0], target_binary, eval_type='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv('predictions_stack.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack\n",
    "sub_df.to_csv('predictions_trick.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv('data/sample_submission.csv')\n",
    "sub_df[\"target\"] = (predictions_bstack*-33.219281 + (1-predictions_bstack)*predictions_nstack)*0.5 + predictions_stack*0.5\n",
    "sub_df.to_csv('predictions_trick&stacking.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
